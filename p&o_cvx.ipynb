{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "import sys\n",
    "import io\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "from torch.utils import data as torch_data\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "from numpy import linalg as LA\n",
    "from sklearn import preprocessing\n",
    "from cvxpylayers.torch.cvxpylayer import CvxpyLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_data = []\n",
    "#inci_matrix = torch.tensor([ [-1,-1,-1,0,0,0,0,0],\n",
    " #   [1,0,0,-1,0,-1,0,0],\n",
    "  #             [0,1,0,1,1,0,-1,0],\n",
    "               \n",
    "   #            [0,0,1,0,-1,0,0,-1],\n",
    "    #           [0,0,0,0,0,1,1,1] \n",
    "     #          ],dtype=torch.float32)\n",
    "\n",
    "\n",
    "inci_matrix = pickle.load( open( \"matrix_{}b{}.p\".format(20,30), \"rb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "#print(inci_matrix)  \n",
    "while len(training_data) != 4000:\n",
    "    \n",
    "    z = torch.tensor([float(random.uniform(0.001, 1)) for i in range(30)])\n",
    "    #z = torch.tensor([e1,e2,e3, e4, e5, e6, e7, e8])\n",
    "    training_data.append(z)\n",
    "training_data = torch.stack(training_data)\n",
    "\n",
    "#model_load = torch.load('shortest_path_proxy_{}.model'.format('Dag'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_net_data_generation(inputs):\n",
    "    \n",
    "    data = []\n",
    "    target = []\n",
    "\n",
    "    N,M  = inputs.shape\n",
    "    \n",
    "    A = torch.rand((M, M))\n",
    "   \n",
    "    print(\"Condition number: \",LA.cond(A))\n",
    "\n",
    "    A_Inv = torch.linalg.inv(A)\n",
    "    alpha = 0.05 # perturbation amplitude\n",
    "\n",
    "    \n",
    "    for i in inputs:\n",
    "        target.append(i)\n",
    "        x = torch.nn.functional.normalize(torch.matmul(A_Inv,i), dim=0, p=2)\n",
    "        data.append(x)\n",
    "        \n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(training_data)\n",
    "x_nn, y_nn = pred_net_data_generation(training_data)\n",
    "\n",
    "x_nn = torch.stack(x_nn)\n",
    "y_nn = torch.stack(y_nn)\n",
    "\n",
    "#x_nn = preprocessing.normalize(x_nn)\n",
    "#y_nn = preprocessing.normalize(y_nn)\n",
    "#print(x_nn[0])\n",
    "print(x_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_random = 9999\n",
    "np.random.seed(seed_random)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_nn, y_nn, test_size=0.25, train_size=0.75, random_state=seed_random, shuffle=True)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=seed_random, shuffle=True)\n",
    "\n",
    "x_train = preprocessing.normalize(x_train)\n",
    "x_test = preprocessing.normalize(x_test)\n",
    "x_cv = preprocessing.normalize(x_cv)\n",
    "y_train = preprocessing.normalize(y_train)\n",
    "y_test = preprocessing.normalize(y_test)\n",
    "y_cv = preprocessing.normalize(y_cv)\n",
    "\n",
    "print('shapes of train, validation, test data', x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "# JK\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming_uniform':\n",
    "                #init.kaiming_uniform(m.weight.data, a=0, mode='fan_in')\n",
    "                #init.kaiming_uniform(m.bias.data, a=0, mode='fan_in')\n",
    "                nn.init.kaiming_uniform_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                init.orthogonal_(m.bias.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find(\n",
    "                'BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'iterations':1000,'batch_size': 128, 'input_size': 30, 'hidden_units_1': 100, 'hidden_units_2':100, 'hidden_units_3': 100, 'hidden_units_4': 100, 'hidden_units_5': 100, 'hidden_units_6': 100,'hidden_units_7': 100,'do_1': 0.2, 'do_2': 0.1, 'do_3': 0.05, 'output_size': 30, 'lr': 0.00001, 'min_lr': 1e-05, 'max_lr': 0.01, 'epochs': 20, 'lr_sched': 'clr', 'lr_sched_mode': 'triangular', 'gamma': 0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "trainset = torch_data.TensorDataset(torch.as_tensor(x_train, dtype=torch.float, device=device), \n",
    "torch.as_tensor(y_train, dtype=torch.float, device=device))\n",
    "train_dl = torch_data.DataLoader(trainset, batch_size=params['batch_size'], drop_last=True)\n",
    "\n",
    "val_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_cv, dtype=torch.float, device=device), \n",
    "torch.as_tensor(y_cv, dtype=torch.float, device=device)), batch_size=params['batch_size'], drop_last=True)\n",
    "\n",
    "test_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_test, dtype=torch.float, device=device), \n",
    "torch.as_tensor(y_test, dtype=torch.float, device=device)), batch_size=params['batch_size'], drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(9999)\n",
    "def get_model():\n",
    "    \"\"\"\n",
    "    creates a PyTorch model. Change the 'params' dict above to \n",
    "    modify the neural net configuration.\n",
    "    \"\"\"\n",
    "    model = torch.nn.Sequential(\n",
    "       torch.nn.Linear(params['input_size'], params['hidden_units_1']),\n",
    "       #torch.nn.ReLU(),\n",
    "      # torch.nn.Linear(params['hidden_units_1'], params['hidden_units_2']),\n",
    "       #torch.nn.ReLU(),\n",
    "      torch.nn.BatchNorm1d(params['hidden_units_1']),\n",
    "         torch.nn.Dropout(p=params['do_1']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_1'], params['hidden_units_2']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_2']),\n",
    "         torch.nn.Dropout(p=params['do_2']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_2'], params['hidden_units_3']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_3']),\n",
    "         torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_3'], params['hidden_units_4']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_4']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_4'], params['hidden_units_5']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_5']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_5'], params['hidden_units_6']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_6']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_6'], params['hidden_units_7']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_7']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_7'], params['output_size']),\n",
    "       #torch.nn.ReLU(),\n",
    "        #torch.nn.Sigmoid()\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def regret(y_true, y_hat,proxy):\n",
    "    \n",
    "    A = inci_matrix\n",
    "     \n",
    "    eps_ = 0.2\n",
    "    #print( \"weight_target {} , predicted weight {}\".format(y_true[0], y_hat[0]))\n",
    "    \n",
    "    y_true_decision = proxy(y_true)\n",
    "    \n",
    "    y_hat_decision = proxy(y_hat)\n",
    "    \n",
    "    #print( \"weight_target decision {} , predicted weight decision {}\".format(y_true_decision[0], y_hat_decision[0]))\n",
    "    \n",
    "    \n",
    "    objective_true =( (y_true * y_true_decision).sum(1) \n",
    "                     #+ torch.tensor([eps_*torch.sum(torch.square(y_true_decision[i])) for i in range((y_true_decision.shape)[0])])\n",
    "                     #+torch.tensor([eps_*torch.sum(y_true_decision[i]*torch.log(y_true_decision[i])) for i in range((y_true_decision.shape)[0])])\n",
    "                     + torch.tensor([eps_*torch.norm(y_true_decision[i], p=2) for i in range((y_true_decision.shape)[0])]) \n",
    "                     )\n",
    "    objective_hat =  ( (y_true * y_hat_decision).sum(1) \n",
    "                   #   + torch.tensor([eps_*torch.sum(torch.square(y_hat_decision[i])) for i in range((y_hat_decision.shape)[0])])\n",
    "                     # +torch.tensor([eps_*torch.sum(y_hat_decision[i]*torch.log(y_hat_decision[i])) for i in range((y_hat_decision.shape)[0])])\n",
    "                      + torch.tensor([eps_*torch.norm(y_hat_decision[i], p=2) for i in range((y_hat_decision.shape)[0])]) \n",
    "                      )\n",
    "    \n",
    "   \n",
    "    regret = torch.mean(objective_hat - objective_true)\n",
    "    #print(regret)\n",
    "   \n",
    "    loss_vec = (y_true * proxy(y_hat)).sum(1)\n",
    "    \n",
    "  \n",
    "    loss = torch.mean(loss_vec)\n",
    "   \n",
    "    return loss, regret\n",
    "\n",
    "def regret_cvxpy(y_true, y_hat,cvx):\n",
    "    A = inci_matrix\n",
    "    \n",
    "    \n",
    "    \n",
    "    eps_ = 0.2\n",
    "    \n",
    "    \n",
    "    y_true_decision = cvx(A,y_true)\n",
    "    y_hat_decision = cvx(A,y_hat)\n",
    "    \n",
    "    #print(y_true_decision)\n",
    "    \n",
    "    #print(y_true_decision[0], y_hat_decision[0])\n",
    "    \n",
    "    \n",
    "    objective_true =( (y_true * y_true_decision).sum(1) \n",
    "                   #  + torch.tensor([eps_*torch.sum(torch.square(y_true_decision[i])) for i in range((y_true_decision.shape)[0])])\n",
    "                    #+torch.tensor([eps_*torch.sum(y_true_decision[i]*torch.log(y_true_decision[i])) for i in range((y_true_decision.shape)[0])])\n",
    "                     + torch.tensor([eps_*torch.norm(y_true_decision[i], p=2) for i in range((y_true_decision.shape)[0])])\n",
    "                     )\n",
    "    #print(objective_true[0])\n",
    "    \n",
    "    objective_hat =  ( (y_true * y_hat_decision).sum(1) \n",
    "                    #  + torch.tensor([eps_*torch.sum(torch.square(y_hat_decision[i])) for i in range((y_hat_decision.shape)[0])])\n",
    "                   # +torch.tensor([eps_*torch.sum(y_hat_decision[i]*torch.log(y_hat_decision[i])) for i in range((y_hat_decision.shape)[0])])\n",
    "                     + torch.tensor([eps_*torch.norm(y_hat_decision[i], p=2) for i in range((y_hat_decision.shape)[0])])  \n",
    "                      )\n",
    "    \n",
    "    regret = torch.mean(objective_hat - objective_true)\n",
    "    return regret\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "initialize_model = True\n",
    "if initialize_model:\n",
    "    init_weights(model, 'xavier')\n",
    "\n",
    "print('model loaded into device=', next(model.parameters()).device)\n",
    "\n",
    "# this is just to capture model summary as string\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "summary(model, input_size=(params['input_size'], ))\n",
    "\n",
    "sys.stdout = old_stdout\n",
    "model_summary = buffer.getvalue()\n",
    "#print('model-summary\\n', model_summary)\n",
    "# later this 'model-summary' string can be written to tensorboard\n",
    "\n",
    "lr_reduce_patience = 20\n",
    "lr_reduce_factor = 0.1\n",
    "\n",
    "loss_fn = regret_cvxpy#nn.MSELoss()  \n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'])#, momentum=0.9, dampening=0, weight_decay=0, nesterov=True)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=params['lr'], alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
    "\n",
    "#if params['lr_sched'] == 'reduce_lr_plateau':\n",
    " #   lr_sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=lr_reduce_factor, patience=lr_reduce_patience, verbose=True, threshold=0.00001, threshold_mode='rel', cooldown=0, min_lr=1e-9, eps=1e-08)\n",
    "#elif params['lr_sched'] == 'clr':\n",
    " #   lr_sched = torch.optim.lr_scheduler.CyclicLR(optimizer, params['min_lr'], params['max_lr'], step_size_up=8*len(train_dl), step_size_down=None, mode=params['lr_sched_mode'], last_epoch=-1, gamma=params['gamma'])\n",
    "\n",
    "#print('lr scheduler type:', lr_sched)\n",
    "#for param_group in optimizer.param_groups:\n",
    " #   print(param_group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_violations(inc_matrix, selected_edges):\n",
    "    #viol = 0\n",
    "    viol_lst = []\n",
    "    n,m = inc_matrix.shape\n",
    "    b = np.zeros(n)\n",
    "    # these are flipped to get 0 for violations, original are the opposite \n",
    "    b[0] = -1\n",
    "    b[n-1] = 1\n",
    "    \n",
    "    batch_size, null = selected_edges.shape\n",
    "    \n",
    "    for i in selected_edges:\n",
    "      #torch.unsqueeze(i,1)\n",
    "      #print(i.shape)\n",
    "      viol = 0\n",
    "      \n",
    "      a = torch.matmul(inci_matrix,i)\n",
    "      #print(a.shape)\n",
    "     # print(a.shape)\n",
    "      #print(b.shape)\n",
    "      c = a - b \n",
    "      for values in c:\n",
    "        viol += torch.abs(values) \n",
    "      \n",
    "      \n",
    "      \n",
    "      for j in i:\n",
    "        if j < 0:\n",
    "          viol += torch.abs(j)\n",
    "        elif j > 1:\n",
    "          viol += torch.abs(j-1)\n",
    "      viol_lst.append(viol)\n",
    "      \n",
    "    #print(viol_lst)\n",
    "      \n",
    "    return  max(viol_lst), sum(viol_lst)/len(viol_lst)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_py(A, pred):\n",
    "   # print('here')\n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    eps = 0.2\n",
    "    \n",
    "    batch_s, edges  = pred.shape\n",
    "   \n",
    "   # print(batch_s)\n",
    "    \n",
    "    n,m = A.shape\n",
    "    target = torch.zeros((batch_s, edges))\n",
    "    \n",
    "    #print(target.shape)\n",
    "    c = cp.Parameter(m)\n",
    "    #c = c.detach().numpy()\n",
    "    b = np.zeros(n)\n",
    "    b[0] = -1\n",
    "    b[n-1] = 1\n",
    "    x = cp.Variable(shape=m)\n",
    "    constr = [x>=0, x <= 1,\n",
    "             A@x == b]\n",
    "    \n",
    "  \n",
    "    #- eps*cp.sum(cp.entr(x))\n",
    "    #+ eps*cp.norm(x,p=2)\n",
    "    #eps*cp.sum(x**2)\n",
    "    problem  = cp.Problem(cp.Minimize(c @ x + eps*cp.norm(x,p=2) ),constr)\n",
    "    cvxlayer = CvxpyLayer(problem, parameters=[c], variables=[x])\n",
    "  \n",
    "   \n",
    "    solutions = lambda z: cvxlayer(z)[0] #, args = {'tol':1e-10})[0]\n",
    "    index = 0\n",
    "    #print(len(pred))\n",
    "    for x in pred:\n",
    "        #print(x)\n",
    "         \n",
    "        #print(x)\n",
    "       # print(x)\n",
    "        \n",
    "        sol = solutions(x)\n",
    "        target[index] = sol\n",
    "        #print(sol)                        \n",
    "        \n",
    "        #print(problem.value)\n",
    "        index += 1\n",
    "    \n",
    "    return torch.Tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 130\n",
    "best = 1000\n",
    "fails = 0\n",
    "flag = False                                           \n",
    "infeasible_count = 0                                                               \n",
    "\n",
    "l1_error_weight_pred = [] # this is to compare distro of predicted arc weights and actual arc weights\n",
    "l2_error_weight_pred = []\n",
    "          \n",
    "avg_constraint_viol_perbatch = []\n",
    "max_constraint_viol_perbatch = []\n",
    "\n",
    "#avg_constraint_viol_cvxpy = []\n",
    "#max_constraint_viol_cvxpy = []\n",
    "\n",
    "train_loss = [] \n",
    "val_loss = []\n",
    "\n",
    "train_regret = []\n",
    "val_regret_lst = []\n",
    "\n",
    "l1_error_decision_pred = []\n",
    "l2_error_decision_pred = []\n",
    "\n",
    "\n",
    "l1_loss_weight = nn.L1Loss()\n",
    "l2_loss_weight = nn.MSELoss()\n",
    "#l1_loss_decision = nn.L1Loss()\n",
    "\n",
    "A = inci_matrix\n",
    "\n",
    "\n",
    "#model_load = torch.load('shortest_path_proxy_{}.model'.format('Dag'))\n",
    "\n",
    "\n",
    "\n",
    "train_iterator = itertools.cycle(train_dl)\n",
    "\n",
    "val_iter = itertools.cycle(test_dl)\n",
    "\n",
    "\n",
    "for iterations in range(0, params['iterations']):\n",
    "  \n",
    "  \n",
    "  \n",
    "  # loop over the dataset multiple times\n",
    "  print(\"---------------------------------------iteration: {}\".format(iterations))\n",
    "  #running_loss = 0.0\n",
    "  #running_viol = 0.0\n",
    "  #stime = time.time()\n",
    "  model.train()\n",
    "  data = next(iter(train_iterator))\n",
    "  inputs, targets  = data\n",
    "  #out_1 = \n",
    "  linear_layer_output = torch.nn.functional.normalize(model(inputs))\n",
    "  #print(linear_layer_output[0])\n",
    "  with torch.no_grad():\n",
    "    l1_weight_val = l1_loss_weight(linear_layer_output,targets)\n",
    "    l2_weight_val = l2_loss_weight(linear_layer_output,targets)\n",
    "    \n",
    "    l1_error_weight_pred.append(l1_weight_val.item())\n",
    "    l2_error_weight_pred.append(l2_weight_val.item())\n",
    "        \n",
    "  #l1_error_decision_pred.append(l1_decision_val)\n",
    "  #\n",
    "\n",
    "  #print(\"l1 decision: {}\".format(l1_decision_val.item()))\n",
    "  print(\"l1 weight: {}\".format(l1_weight_val.item()))\n",
    "                  \n",
    "  regret = loss_fn(targets, linear_layer_output, cvx_py)\n",
    "  regret.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "  \n",
    "  #print(\"regret\", regret.item())\n",
    "  \n",
    "  train_regret.append(regret.item())\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  model.eval()\n",
    "  val_data = next(iter(val_iter))\n",
    "  \n",
    "  val_inputs, val_targets = val_data\n",
    "  with torch.no_grad():\n",
    "   # print(\"eval----------\")\n",
    "    linear_layer_output_val = torch.nn.functional.normalize(model(val_inputs))\n",
    "    val_regret = loss_fn(val_targets, linear_layer_output_val, cvx_py)\n",
    "    val_regret_lst.append(val_regret.item())\n",
    "    #print(\"eval done------\")\n",
    "\n",
    "  print('train regret : {}, valid regret : {} '.format(regret, val_regret))\n",
    "  \n",
    "  if val_regret < best - 1e-4:\n",
    "    if val_regret > 0:\n",
    "      print(\"\\n UPDATE \\n\")\n",
    "      best_model = copy.deepcopy(model)\n",
    "      fails = 0\n",
    "      best = val_regret\n",
    "  else:\n",
    "    fails = fails + 1\n",
    "  if fails > patience:\n",
    "    print(\"Early Stopping. Valid hasn't improved for {}\".format(patience))\n",
    "    flag = True\n",
    "  if flag:\n",
    "    break\n",
    "  val_regret = 0\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "print('Finished Training')\n",
    "print(\"number of infeasible solutions: \",infeasible_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_py_test(A, pred):\n",
    "   \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    #eps = 0.7\n",
    "    \n",
    "    batch_s, edges  = pred.shape\n",
    "   \n",
    "    \n",
    "    n,m = A.shape\n",
    "    target = torch.zeros((batch_s, edges))\n",
    "    \n",
    "    #print(target.shape)\n",
    "    c = cp.Parameter(m)\n",
    "    #c = c.detach().numpy()\n",
    "    b = np.zeros(n)\n",
    "    b[0] = -1\n",
    "    b[n-1] = 1\n",
    "    x = cp.Variable(shape=m)\n",
    "    constr = [x>=0, x <= 1,\n",
    "             A*x == b]\n",
    "    \n",
    "  \n",
    "    #- eps*cp.sum(cp.entr(x))\n",
    "    #+ eps*cp.norm(x,p=2)\n",
    "    #eps*cp.sum(x**2)\n",
    "    problem  = cp.Problem(cp.Minimize(c @ x ),constr)\n",
    "    cvxlayer = CvxpyLayer(problem, parameters=[c], variables=[x])\n",
    "  \n",
    "   \n",
    "    solutions = lambda z: cvxlayer(z)[0] #, args = {'tol':1e-10})[0]\n",
    "    index = 0\n",
    "    for x in pred:\n",
    "        \n",
    "         \n",
    "        #print(x)\n",
    "       # print(x)\n",
    "        \n",
    "            #sol = solutions(x)\n",
    "        target[index] = solutions(x)                            \n",
    "        \n",
    "        #print(problem.value)\n",
    "        index += 1\n",
    "    \n",
    "    return torch.Tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = itertools.cycle(test_dl)\n",
    "test_l2 = nn.MSELoss() \n",
    "test_l1 = nn.L1Loss() \n",
    "\n",
    "\n",
    "l2_weights = []\n",
    "l2_decision = []\n",
    "test_viol = []\n",
    "l1_weights = []\n",
    "inf_time = []\n",
    "\n",
    "#optimal_path_acc = []\n",
    "\n",
    "i =0 \n",
    "for inputs, targets in test_iter:\n",
    "    \n",
    "    start = time.process_time()\n",
    "    linear_layer_output = torch.nn.functional.normalize(model(inputs))\n",
    "    selected_edges = cvx_py_test(inci_matrix,linear_layer_output)\n",
    "    inf_time.append(time.process_time() - start)\n",
    "    \n",
    "    error_w = test_l2(linear_layer_output,targets)\n",
    "    l2_weights.append(error_w)\n",
    "    \n",
    "    error_w1 = test_l1(linear_layer_output,targets)\n",
    "    l1_weights.append(error_w1)\n",
    "\n",
    "    selected_edges_targets = cvx_py_test(inci_matrix,targets)\n",
    "    error_d = test_l2(selected_edges,selected_edges_targets)\n",
    "    l2_decision.append(error_d)\n",
    "    \n",
    "    vio = calc_violations(inci_matrix,selected_edges.detach())[1]\n",
    "    test_viol.append(vio)\n",
    "    #selected_edges = model_load(linear_layer_output)\n",
    "    #selected_edges_targets = cvx_py(A,linear_layer_output) \n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "print(\"l2 weights\", sum(l2_weights)/len(l2_weights))\n",
    "print(\"l2 decision\", sum(l2_decision)/len(l2_decision))\n",
    "print(\"inf time\", sum(inf_time)/len(inf_time))\n",
    "print(\"violations\", sum(test_viol)/len(test_viol))\n",
    "print(\"l1 weights\", sum(l1_weights)/len(l1_weights))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_regret,color='r', label='train loss')\n",
    "plt.plot(val_regret_lst, color='g', label='val loss')\n",
    "\n",
    "plt.xlabel(\"regret\")\n",
    "plt.ylabel(\"iterations\")\n",
    "plt.title(\"regret\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(l1_error_weight_pred)):\n",
    " #   l1_error_weight_pred[i] = l1_error_weight_pred[i].cpu().detach().numpy()\n",
    "\n",
    "plt.plot(l1_error_weight_pred)\n",
    "plt.ylabel('l1 weight error')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"l1 weight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(len(l1_error_weight_pred)):\n",
    " #   l1_error_weight_pred[i] = l1_error_weight_pred[i].cpu().detach().numpy()\n",
    "\n",
    "plt.plot(l2_error_weight_pred)\n",
    "plt.ylabel('l2 weight error')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"l2 weight\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5842daea53dd272ed90462daeb5f210458e1b3f8f882f697cb32ab8b4c14a531"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
