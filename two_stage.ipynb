{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import cvxpy as cp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from itertools import chain\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchsummary import summary\n",
    "import sys\n",
    "import io\n",
    "import copy\n",
    "import time\n",
    "import torch\n",
    "from torch.utils import data as torch_data\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "from numpy import linalg as LA\n",
    "from sklearn import preprocessing\n",
    "from cvxpylayers.torch.cvxpylayer import CvxpyLayer\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "\n",
    "#inci_matrix = torch.tensor([ [-1,-1,-1,0,0,0,0,0],\n",
    "  # [1,0,0,-1,0,-1,0,0],\n",
    "             #  [0,1,0,1,1,0,-1,0],\n",
    "               \n",
    "             #  [0,0,1,0,-1,0,0,-1],\n",
    "            #   [0,0,0,0,0,1,1,1] \n",
    "             #  ],dtype=torch.float32)\n",
    "inci_matrix = pickle.load( open( \"matrix_{}b{}.p\".format(80,130), \"rb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "while len(training_data) != 4000:\n",
    "    \n",
    "    z = torch.tensor([float(random.uniform(0.001, 1)) for i in range(130)])\n",
    "    #z = torch.tensor([e1,e2,e3, e4, e5, e6, e7, e8])\n",
    "    training_data.append(z)\n",
    "training_data = torch.stack(training_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvx_py(A, pred):\n",
    "   \n",
    "    warnings.filterwarnings('ignore')\n",
    "    \n",
    "    \n",
    "    eps = 0.5\n",
    "    \n",
    "    batch_s, edges  = pred.shape\n",
    "   \n",
    "    \n",
    "    n,m = A.shape\n",
    "    target = torch.zeros((batch_s, edges))\n",
    "    \n",
    "    #print(target.shape)\n",
    "    c = cp.Parameter(m)\n",
    "    #c = c.detach().numpy()\n",
    "    b = np.zeros(n)\n",
    "    b[0] = -1\n",
    "    b[n-1] = 1\n",
    "    x = cp.Variable(shape=m)\n",
    "    constr = [x>=0, x <= 1,\n",
    "             A*x == b]\n",
    "    \n",
    "  \n",
    "    \n",
    "    problem  = cp.Problem(cp.Minimize(c @ x ),constr)\n",
    "    cvxlayer = CvxpyLayer(problem, parameters=[c], variables=[x])\n",
    "  \n",
    "   \n",
    "    solutions = lambda z: cvxlayer(z)[0] #, args = {'tol':1e-10})[0]\n",
    "    index = 0\n",
    "    for x in pred:\n",
    "        \n",
    "         \n",
    "        #print(x)\n",
    "       # print(x)\n",
    "        \n",
    "            #sol = solutions(x)\n",
    "        target[index] = solutions(x)\n",
    "        \n",
    "        #print(problem.value)\n",
    "        index += 1\n",
    "    \n",
    "    return torch.Tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_violations(inc_matrix, selected_edges):\n",
    "    #viol = 0\n",
    "    viol_lst = []\n",
    "    n,m = inc_matrix.shape\n",
    "    b = np.zeros(n)\n",
    "    # these are flipped to get 0 for violations, original are the opposite \n",
    "    b[0] = -1\n",
    "    b[n-1] = 1\n",
    "    \n",
    "    batch_size, null = selected_edges.shape\n",
    "    \n",
    "    for i in selected_edges:\n",
    "      #torch.unsqueeze(i,1)\n",
    "      #print(i.shape)\n",
    "      viol = 0\n",
    "      \n",
    "      a = torch.matmul(inci_matrix,i)\n",
    "      #print(a.shape)\n",
    "     # print(a.shape)\n",
    "      #print(b.shape)\n",
    "      c = a - b \n",
    "      for values in c:\n",
    "        viol += torch.abs(values) \n",
    "      \n",
    "      \n",
    "      \n",
    "      for j in i:\n",
    "        if j < 0:\n",
    "          viol += torch.abs(j)\n",
    "        elif j > 1:\n",
    "          viol += torch.abs(j-1)\n",
    "      viol_lst.append(viol)\n",
    "      \n",
    "    #print(viol_lst)\n",
    "      \n",
    "    return  max(viol_lst), sum(viol_lst)/len(viol_lst)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret_cvxpy(y_true, y_hat,cvx):\n",
    "    A = inci_matrix    \n",
    "    y_true_decision = cvx(A,y_true)\n",
    "    y_hat_decision = cvx(A,y_hat)\n",
    "    \n",
    "    \n",
    "    objective_true = (y_true * y_true_decision).sum(1)\n",
    "    objective_hat =  (y_true * y_hat_decision).sum(1)\n",
    "    \n",
    "   # print(objective_true.shape)\n",
    "   # print(objective_hat.shape)\n",
    "    regret = torch.mean(objective_hat - objective_true)\n",
    "   # print(regret.shape)\n",
    "    return regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_net_data_generation(inputs):\n",
    "    \n",
    "    data = []\n",
    "    target = []\n",
    "\n",
    "    N,M  = inputs.shape\n",
    "    \n",
    "    A = torch.rand((M, M))\n",
    "   \n",
    "    print(\"Condition number: \",LA.cond(A))\n",
    "\n",
    "    A_Inv = torch.linalg.inv(A)\n",
    "    alpha = 0.05 # perturbation amplitude\n",
    "\n",
    "    \n",
    "    for i in inputs:\n",
    "        target.append(i)\n",
    "        x = torch.nn.functional.normalize(torch.matmul(A_Inv,i), dim=0, p=2)\n",
    "        data.append(x)\n",
    "        \n",
    "    \n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition number:  555871.5\n"
     ]
    }
   ],
   "source": [
    "#print(training_data)\n",
    "x_nn, y_nn = pred_net_data_generation(training_data)\n",
    "\n",
    "x_nn = torch.stack(x_nn)\n",
    "y_nn = torch.stack(y_nn)\n",
    "\n",
    "x_nn = preprocessing.normalize(x_nn)\n",
    "y_nn = preprocessing.normalize(y_nn)\n",
    "#print(x_nn[0])\n",
    "#print(x_nn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes of train, validation, test data (2400, 130) (2400, 130) (600, 130) (600, 130) (1000, 130) (1000, 130)\n"
     ]
    }
   ],
   "source": [
    "seed_random = 9999\n",
    "np.random.seed(seed_random)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_nn, y_nn, test_size=0.25, train_size=0.75, random_state=seed_random, shuffle=True)\n",
    "x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.2, train_size=0.8, random_state=seed_random, shuffle=True)\n",
    "\n",
    "print('shapes of train, validation, test data', x_train.shape, y_train.shape, x_cv.shape, y_cv.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'iterations': 1000,'batch_size': 100, 'input_size': 130, 'hidden_units_1': 100, 'hidden_units_2': 200, 'hidden_units_3': 250, 'hidden_units_4': 300, 'hidden_units_5': 250, 'hidden_units_6': 200, 'hidden_units_7': 100, 'do_1': 0.2, 'do_2': 0.1, 'do_3': 0.05, 'output_size': 130, 'lr': 0.001, 'min_lr': 1e-05, 'max_lr': 0.001, 'epochs': 30, 'lr_sched': 'clr', 'lr_sched_mode': 'triangular', 'gamma': 0.95}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "trainset = torch_data.TensorDataset(torch.as_tensor(x_train, dtype=torch.float, device=device), \n",
    "torch.as_tensor(y_train, dtype=torch.float, device=device))\n",
    "train_dl = torch_data.DataLoader(trainset, batch_size=params['batch_size'], drop_last=True)\n",
    "\n",
    "val_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_cv, dtype=torch.float, device=device), \n",
    "torch.as_tensor(y_cv, dtype=torch.float, device=device)), batch_size=params['batch_size'], drop_last=True)\n",
    "\n",
    "test_dl = torch_data.DataLoader(torch_data.TensorDataset(torch.as_tensor(x_test, dtype=torch.float, device=device), \n",
    "torch.as_tensor(y_test, dtype=torch.float, device=device)), batch_size=params['batch_size'], drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "                #init.kaiming_uniform(m.weight.data, a=0, mode='fan_in')\n",
    "                #init.kaiming_uniform(m.bias.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "                #init.orthogonal_(m.bias.data, gain=init_gain)\n",
    "                \n",
    "            elif init_type == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(m.weight.data, mode='fan_in', nonlinearity='relu')\n",
    "\n",
    "            \n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find(\n",
    "                'BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialize network with xavier\n",
      "model loaded into device= cpu\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(9999)\n",
    "def get_model():\n",
    "    \"\"\"\n",
    "    creates a PyTorch model. Change the 'params' dict above to \n",
    "    modify the neural net configuration.\n",
    "    \"\"\"\n",
    "    model = torch.nn.Sequential(\n",
    "        torch.nn.Linear(params['input_size'], params['hidden_units_1']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_1']),\n",
    "         torch.nn.Dropout(p=params['do_1']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_1'], params['hidden_units_2']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_2']),\n",
    "         torch.nn.Dropout(p=params['do_2']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_2'], params['hidden_units_3']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_3']),\n",
    "         torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_3'], params['hidden_units_4']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_4']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_4'], params['hidden_units_5']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_5']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_5'], params['hidden_units_6']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_6']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_6'], params['hidden_units_7']),\n",
    "        torch.nn.BatchNorm1d(params['hidden_units_7']),\n",
    "        torch.nn.Dropout(p=params['do_3']),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(params['hidden_units_7'], params['output_size']),\n",
    "       # torch.nn.Sigmoid(),\n",
    "        # torch.nn.Softplus(),\n",
    "    )\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "#def mse_viol(pred, target):\n",
    "    \n",
    "\n",
    "model = get_model()\n",
    "#best_model = copy.deepcopy(model)\n",
    "initialize_model = True\n",
    "if initialize_model:\n",
    "    init_weights(model, 'xavier')\n",
    "\n",
    "print('model loaded into device=', next(model.parameters()).device)\n",
    "\n",
    "# this is just to capture model summary as string\n",
    "old_stdout = sys.stdout\n",
    "sys.stdout = buffer = io.StringIO()\n",
    "\n",
    "summary(model, input_size=(params['input_size'], ))\n",
    "\n",
    "sys.stdout = old_stdout\n",
    "model_summary = buffer.getvalue()\n",
    "#print('model-summary\\n', model_summary)\n",
    "# later this 'model-summary' string can be written to tensorboard\n",
    "\n",
    "#lr_reduce_patience = 20\n",
    "#lr_reduce_factor = 0.1\n",
    "\n",
    "loss_fn = nn.MSELoss()  \n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=params['lr'], momentum=0.9, dampening=0, weight_decay=0, nesterov=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------iteration: 0\n",
      "loss 0.007890566252171993\n",
      "eval----------\n",
      "regret 0.5002570748329163\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 1\n",
      "loss 0.007891401648521423\n",
      "eval----------\n",
      "regret 0.526167631149292\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 2\n",
      "loss 0.007862705737352371\n",
      "eval----------\n",
      "regret 0.497384250164032\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 3\n",
      "loss 0.007876514457166195\n",
      "eval----------\n",
      "regret 0.4744223356246948\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 4\n",
      "loss 0.007855872623622417\n",
      "eval----------\n",
      "regret 0.46234431862831116\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 5\n",
      "loss 0.00788694154471159\n",
      "eval----------\n",
      "regret 0.4610692262649536\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 6\n",
      "loss 0.00787959061563015\n",
      "eval----------\n",
      "regret 0.4443582594394684\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 7\n",
      "loss 0.007893554866313934\n",
      "eval----------\n",
      "regret 0.478168249130249\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 8\n",
      "loss 0.00788701418787241\n",
      "eval----------\n",
      "regret 0.46163642406463623\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 9\n",
      "loss 0.007882021367549896\n",
      "eval----------\n",
      "regret 0.4744308888912201\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 10\n",
      "loss 0.007890859618782997\n",
      "eval----------\n",
      "regret 0.4623434841632843\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 11\n",
      "loss 0.007882677949965\n",
      "eval----------\n",
      "regret 0.4118915796279907\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 12\n",
      "loss 0.007880942896008492\n",
      "eval----------\n",
      "regret 0.40154579281806946\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 13\n",
      "loss 0.007870357483625412\n",
      "eval----------\n",
      "regret 0.43352776765823364\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 14\n",
      "loss 0.007853171788156033\n",
      "eval----------\n",
      "regret 0.4190835952758789\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 15\n",
      "loss 0.007869169116020203\n",
      "eval----------\n",
      "regret 0.42559418082237244\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 16\n",
      "loss 0.007871284149587154\n",
      "eval----------\n",
      "regret 0.42205917835235596\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 17\n",
      "loss 0.007855871692299843\n",
      "eval----------\n",
      "regret 0.42006224393844604\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 18\n",
      "loss 0.007877405732870102\n",
      "eval----------\n",
      "regret 0.41056087613105774\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 19\n",
      "loss 0.007848010398447514\n",
      "eval----------\n",
      "regret 0.44235464930534363\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 20\n",
      "loss 0.007851080037653446\n",
      "eval----------\n",
      "regret 0.46163618564605713\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 21\n",
      "loss 0.00784851424396038\n",
      "eval----------\n",
      "regret 0.43101388216018677\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 22\n",
      "loss 0.007855966687202454\n",
      "eval----------\n",
      "regret 0.46234241127967834\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 23\n",
      "loss 0.007857107557356358\n",
      "eval----------\n",
      "regret 0.4610776901245117\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 24\n",
      "loss 0.00783473625779152\n",
      "eval----------\n",
      "regret 0.4443209767341614\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 25\n",
      "loss 0.00782886054366827\n",
      "eval----------\n",
      "regret 0.47813090682029724\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 26\n",
      "loss 0.00783070269972086\n",
      "eval----------\n",
      "regret 0.4616313576698303\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 27\n",
      "loss 0.007838563993573189\n",
      "eval----------\n",
      "regret 0.4744313359260559\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 28\n",
      "loss 0.007841483689844608\n",
      "eval----------\n",
      "regret 0.46234339475631714\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 29\n",
      "loss 0.007845265790820122\n",
      "eval----------\n",
      "regret 0.4610680639743805\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 30\n",
      "loss 0.007823900319635868\n",
      "eval----------\n",
      "regret 0.444357693195343\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 31\n",
      "loss 0.007816207595169544\n",
      "eval----------\n",
      "regret 0.4781462848186493\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 32\n",
      "loss 0.007825031876564026\n",
      "eval----------\n",
      "regret 0.4616299867630005\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 33\n",
      "loss 0.007836999371647835\n",
      "eval----------\n",
      "regret 0.4744318723678589\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 34\n",
      "loss 0.007821273058652878\n",
      "eval----------\n",
      "regret 0.4623420834541321\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 35\n",
      "loss 0.00782177597284317\n",
      "eval----------\n",
      "regret 0.4610676169395447\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 36\n",
      "loss 0.007807186339050531\n",
      "eval----------\n",
      "regret 0.4443579614162445\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 37\n",
      "loss 0.00781167671084404\n",
      "eval----------\n",
      "regret 0.4781671166419983\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 38\n",
      "loss 0.007801060564815998\n",
      "eval----------\n",
      "regret 0.4616210162639618\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 39\n",
      "loss 0.007798975799232721\n",
      "eval----------\n",
      "regret 0.47442421317100525\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 40\n",
      "loss 0.007789634168148041\n",
      "eval----------\n",
      "regret 0.4623478055000305\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 41\n",
      "loss 0.007791250478476286\n",
      "eval----------\n",
      "regret 0.46106505393981934\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 42\n",
      "loss 0.007801286410540342\n",
      "eval----------\n",
      "regret 0.4443604350090027\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 43\n",
      "loss 0.007788778282701969\n",
      "eval----------\n",
      "regret 0.4781665802001953\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 44\n",
      "loss 0.0077723846770823\n",
      "eval----------\n",
      "regret 0.4616354703903198\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 45\n",
      "loss 0.007752973586320877\n",
      "eval----------\n",
      "regret 0.47442999482154846\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 46\n",
      "loss 0.0077665685676038265\n",
      "eval----------\n",
      "regret 0.46234238147735596\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 47\n",
      "loss 0.007782572414726019\n",
      "eval----------\n",
      "regret 0.4610450863838196\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 48\n",
      "loss 0.007763967849314213\n",
      "eval----------\n",
      "regret 0.444358229637146\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 49\n",
      "loss 0.00777445500716567\n",
      "eval----------\n",
      "regret 0.47816750407218933\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 50\n",
      "loss 0.007773933466523886\n",
      "eval----------\n",
      "regret 0.46163642406463623\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 51\n",
      "loss 0.007781606633216143\n",
      "eval----------\n",
      "regret 0.47439154982566833\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 52\n",
      "loss 0.007761930115520954\n",
      "eval----------\n",
      "regret 0.46234285831451416\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 53\n",
      "loss 0.007765756454318762\n",
      "eval----------\n",
      "regret 0.4610680639743805\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 54\n",
      "loss 0.007737498264759779\n",
      "eval----------\n",
      "regret 0.44435882568359375\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 55\n",
      "loss 0.0077640400268137455\n",
      "eval----------\n",
      "regret 0.4781428277492523\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 56\n",
      "loss 0.007780719548463821\n",
      "eval----------\n",
      "regret 0.46163642406463623\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 57\n",
      "loss 0.007756976876407862\n",
      "eval----------\n",
      "regret 0.47440800070762634\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 58\n",
      "loss 0.007737959735095501\n",
      "eval----------\n",
      "regret 0.46234196424484253\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 59\n",
      "loss 0.0077453488484025\n",
      "eval----------\n",
      "regret 0.4610682427883148\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 60\n",
      "loss 0.007749028038233519\n",
      "eval----------\n",
      "regret 0.44438156485557556\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 61\n",
      "loss 0.007722389884293079\n",
      "eval----------\n",
      "regret 0.47820210456848145\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 62\n",
      "loss 0.0077395373955369\n",
      "eval----------\n",
      "regret 0.46161559224128723\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 63\n",
      "loss 0.007734742946922779\n",
      "eval----------\n",
      "regret 0.47442924976348877\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 64\n",
      "loss 0.007735526654869318\n",
      "eval----------\n",
      "regret 0.46234267950057983\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 65\n",
      "loss 0.0077249580062925816\n",
      "eval----------\n",
      "regret 0.4610680043697357\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 66\n",
      "loss 0.007734275422990322\n",
      "eval----------\n",
      "regret 0.44435805082321167\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 67\n",
      "loss 0.00772296916693449\n",
      "eval----------\n",
      "regret 0.4781649112701416\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 68\n",
      "loss 0.007711063604801893\n",
      "eval----------\n",
      "regret 0.4616135060787201\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 69\n",
      "loss 0.007718169596046209\n",
      "eval----------\n",
      "regret 0.47442883253097534\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 70\n",
      "loss 0.007736868690699339\n",
      "eval----------\n",
      "regret 0.46234267950057983\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 71\n",
      "loss 0.007720586843788624\n",
      "eval----------\n",
      "regret 0.46106788516044617\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 72\n",
      "loss 0.007715125568211079\n",
      "eval----------\n",
      "regret 0.44435492157936096\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 73\n",
      "loss 0.007685906253755093\n",
      "eval----------\n",
      "regret 0.47818267345428467\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 74\n",
      "loss 0.007713500875979662\n",
      "eval----------\n",
      "regret 0.4616345167160034\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 75\n",
      "loss 0.007726750802248716\n",
      "eval----------\n",
      "regret 0.47441861033439636\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 76\n",
      "loss 0.007689568214118481\n",
      "eval----------\n",
      "regret 0.4623448848724365\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 77\n",
      "loss 0.0076956492848694324\n",
      "eval----------\n",
      "regret 0.46107086539268494\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 78\n",
      "loss 0.007678904104977846\n",
      "eval----------\n",
      "regret 0.4443407356739044\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 79\n",
      "loss 0.0076833805069327354\n",
      "eval----------\n",
      "regret 0.47815796732902527\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 80\n",
      "loss 0.0076955221593379974\n",
      "eval----------\n",
      "regret 0.46164000034332275\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 81\n",
      "loss 0.007689299993216991\n",
      "eval----------\n",
      "regret 0.47444140911102295\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 82\n",
      "loss 0.007656909059733152\n",
      "eval----------\n",
      "regret 0.46235039830207825\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 83\n",
      "loss 0.0076621477492153645\n",
      "eval----------\n",
      "regret 0.4610733687877655\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 84\n",
      "loss 0.007675611414015293\n",
      "eval----------\n",
      "regret 0.44436171650886536\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 85\n",
      "loss 0.007682715076953173\n",
      "eval----------\n",
      "regret 0.478173166513443\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 86\n",
      "loss 0.007681619375944138\n",
      "eval----------\n",
      "regret 0.46163544058799744\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 87\n",
      "loss 0.007664112839847803\n",
      "eval----------\n",
      "regret 0.47442910075187683\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 88\n",
      "loss 0.007659043651074171\n",
      "eval----------\n",
      "regret 0.46234363317489624\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 89\n",
      "loss 0.007664445322006941\n",
      "eval----------\n",
      "regret 0.46107470989227295\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 90\n",
      "loss 0.0076723298989236355\n",
      "eval----------\n",
      "regret 0.4443485140800476\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 91\n",
      "loss 0.007667322177439928\n",
      "eval----------\n",
      "regret 0.4781675338745117\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 92\n",
      "loss 0.007632216438651085\n",
      "eval----------\n",
      "regret 0.4342908561229706\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 93\n",
      "loss 0.007650812156498432\n",
      "eval----------\n",
      "regret 0.41996583342552185\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 94\n",
      "loss 0.007642237935215235\n",
      "eval----------\n",
      "regret 0.3970726728439331\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 95\n",
      "loss 0.007646833546459675\n",
      "eval----------\n",
      "regret 0.38953879475593567\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 96\n",
      "loss 0.007659721188247204\n",
      "eval----------\n",
      "regret 0.389252632856369\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 97\n",
      "loss 0.007653100416064262\n",
      "eval----------\n",
      "regret 0.4161897897720337\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 98\n",
      "loss 0.007649529725313187\n",
      "eval----------\n",
      "regret 0.3965610861778259\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 99\n",
      "loss 0.007648535072803497\n",
      "eval----------\n",
      "regret 0.4199649393558502\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 100\n",
      "loss 0.007638786919414997\n",
      "eval----------\n",
      "regret 0.3970733880996704\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 101\n",
      "loss 0.007645363453775644\n",
      "eval----------\n",
      "regret 0.3888232409954071\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 102\n",
      "loss 0.00762952258810401\n",
      "eval----------\n",
      "regret 0.38667288422584534\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 103\n",
      "loss 0.007606443949043751\n",
      "eval----------\n",
      "regret 0.4161897897720337\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 104\n",
      "loss 0.007627696264535189\n",
      "eval----------\n",
      "regret 0.3963080644607544\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 105\n",
      "loss 0.00762128783389926\n",
      "eval----------\n",
      "regret 0.41999924182891846\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 106\n",
      "loss 0.007629795931279659\n",
      "eval----------\n",
      "regret 0.3939414918422699\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 107\n",
      "loss 0.007606667932122946\n",
      "eval----------\n",
      "regret 0.3724719285964966\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 108\n",
      "loss 0.0076059456914663315\n",
      "eval----------\n",
      "regret 0.3219050467014313\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 109\n",
      "loss 0.0075997402891516685\n",
      "eval----------\n",
      "regret 0.3297496736049652\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 110\n",
      "loss 0.007609274238348007\n",
      "eval----------\n",
      "regret 0.31686878204345703\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 111\n",
      "loss 0.007616395130753517\n",
      "eval----------\n",
      "regret 0.3327869474887848\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 112\n",
      "loss 0.007591769099235535\n",
      "eval----------\n",
      "regret 0.32141757011413574\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 113\n",
      "loss 0.0076116458512842655\n",
      "eval----------\n",
      "regret 0.30721184611320496\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 114\n",
      "loss 0.007594750262796879\n",
      "eval----------\n",
      "regret 0.32107943296432495\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 115\n",
      "loss 0.007603590842336416\n",
      "eval----------\n",
      "regret 0.32401955127716064\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 116\n",
      "loss 0.007578426040709019\n",
      "eval----------\n",
      "regret 0.3173987865447998\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 117\n",
      "loss 0.007570325396955013\n",
      "eval----------\n",
      "regret 0.3327958285808563\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 118\n",
      "loss 0.007585559971630573\n",
      "eval----------\n",
      "regret 0.3206130564212799\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 119\n",
      "loss 0.0075842272490262985\n",
      "eval----------\n",
      "regret 0.3054535686969757\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 120\n",
      "loss 0.007573481649160385\n",
      "eval----------\n",
      "regret 0.3225708305835724\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 121\n",
      "loss 0.007558524142950773\n",
      "eval----------\n",
      "regret 0.3243541717529297\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 122\n",
      "loss 0.007566018961369991\n",
      "eval----------\n",
      "regret 0.31488025188446045\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 123\n",
      "loss 0.007592863868921995\n",
      "eval----------\n",
      "regret 0.33461883664131165\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 124\n",
      "loss 0.007566120009869337\n",
      "eval----------\n",
      "regret 0.32047736644744873\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.008 \n",
      "------------------------------------iteration: 125\n",
      "loss 0.007576250471174717\n",
      "eval----------\n",
      "regret 0.2982493042945862\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 126\n",
      "loss 0.007547316141426563\n",
      "eval----------\n",
      "regret 0.32163089513778687\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 127\n",
      "loss 0.007552732713520527\n",
      "eval----------\n",
      "regret 0.2950158417224884\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 128\n",
      "loss 0.007556047290563583\n",
      "eval----------\n",
      "regret 0.2486715465784073\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 129\n",
      "loss 0.007568394765257835\n",
      "eval----------\n",
      "regret 0.27323025465011597\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 130\n",
      "loss 0.007545746397227049\n",
      "eval----------\n",
      "regret 0.2536327838897705\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 131\n",
      "loss 0.007529939990490675\n",
      "eval----------\n",
      "regret 0.22625166177749634\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 132\n",
      "loss 0.007547678891569376\n",
      "eval----------\n",
      "regret 0.24865728616714478\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 133\n",
      "loss 0.007551103830337524\n",
      "eval----------\n",
      "regret 0.3125689625740051\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 134\n",
      "loss 0.007529894355684519\n",
      "eval----------\n",
      "regret 0.23712852597236633\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 135\n",
      "loss 0.00752283027395606\n",
      "eval----------\n",
      "regret 0.2594664692878723\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 136\n",
      "loss 0.007500746287405491\n",
      "eval----------\n",
      "regret 0.25414708256721497\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 137\n",
      "loss 0.007546849083155394\n",
      "eval----------\n",
      "regret 0.22791418433189392\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 138\n",
      "loss 0.007545831613242626\n",
      "eval----------\n",
      "regret 0.24985286593437195\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 139\n",
      "loss 0.007523279637098312\n",
      "eval----------\n",
      "regret 0.2556923031806946\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 140\n",
      "loss 0.007509082555770874\n",
      "eval----------\n",
      "regret 0.23138421773910522\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 141\n",
      "loss 0.007520819548517466\n",
      "eval----------\n",
      "regret 0.2594688832759857\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 142\n",
      "loss 0.0075209662318229675\n",
      "eval----------\n",
      "regret 0.2537687122821808\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 143\n",
      "loss 0.007536487188190222\n",
      "eval----------\n",
      "regret 0.22580188512802124\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 144\n",
      "loss 0.007508701644837856\n",
      "eval----------\n",
      "regret 0.24866104125976562\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 145\n",
      "loss 0.007499940227717161\n",
      "eval----------\n",
      "regret 0.25569474697113037\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 146\n",
      "loss 0.007500829175114632\n",
      "eval----------\n",
      "regret 0.2333352118730545\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 147\n",
      "loss 0.007530457805842161\n",
      "eval----------\n",
      "regret 0.25946253538131714\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 148\n",
      "loss 0.00748771708458662\n",
      "eval----------\n",
      "regret 0.25276967883110046\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 149\n",
      "loss 0.007508414797484875\n",
      "eval----------\n",
      "regret 0.22715528309345245\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 150\n",
      "loss 0.007482894696295261\n",
      "eval----------\n",
      "regret 0.24936255812644958\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 151\n",
      "loss 0.0075013539753854275\n",
      "eval----------\n",
      "regret 0.2603565454483032\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 152\n",
      "loss 0.007505292072892189\n",
      "eval----------\n",
      "regret 0.25327062606811523\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 153\n",
      "loss 0.007504348177462816\n",
      "eval----------\n",
      "regret 0.27543964982032776\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 154\n",
      "loss 0.007482434157282114\n",
      "eval----------\n",
      "regret 0.27714869379997253\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 155\n",
      "loss 0.007482369430363178\n",
      "eval----------\n",
      "regret 0.24075473845005035\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 156\n",
      "loss 0.007467308547347784\n",
      "eval----------\n",
      "regret 0.2501072585582733\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 157\n",
      "loss 0.007502880413085222\n",
      "eval----------\n",
      "regret 0.257424920797348\n",
      "eval done------\n",
      " train loss: 0.008, valid loss: 0.007 \n",
      "------------------------------------iteration: 158\n",
      "loss 0.007478311192244291\n",
      "eval----------\n",
      "regret 0.23138010501861572\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 159\n",
      "loss 0.0074633038602769375\n",
      "eval----------\n",
      "regret 0.25946685671806335\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 160\n",
      "loss 0.007484466768801212\n",
      "eval----------\n",
      "regret 0.25363871455192566\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 161\n",
      "loss 0.007480544503778219\n",
      "eval----------\n",
      "regret 0.22536534070968628\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 162\n",
      "loss 0.007472412195056677\n",
      "eval----------\n",
      "regret 0.24865438044071198\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 163\n",
      "loss 0.0074745663441717625\n",
      "eval----------\n",
      "regret 0.25644052028656006\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 164\n",
      "loss 0.007451723329722881\n",
      "eval----------\n",
      "regret 0.23260578513145447\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 165\n",
      "loss 0.007453737314790487\n",
      "eval----------\n",
      "regret 0.2598802447319031\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 166\n",
      "loss 0.0074844746850430965\n",
      "eval----------\n",
      "regret 0.254403680562973\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 167\n",
      "loss 0.007466117851436138\n",
      "eval----------\n",
      "regret 0.2282285839319229\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 168\n",
      "loss 0.007426223251968622\n",
      "eval----------\n",
      "regret 0.24866098165512085\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 169\n",
      "loss 0.007451721932739019\n",
      "eval----------\n",
      "regret 0.2572188079357147\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 170\n",
      "loss 0.0074621583335101604\n",
      "eval----------\n",
      "regret 0.23457445204257965\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 171\n",
      "loss 0.007454243488609791\n",
      "eval----------\n",
      "regret 0.2584701180458069\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 172\n",
      "loss 0.007425242103636265\n",
      "eval----------\n",
      "regret 0.2532702684402466\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 173\n",
      "loss 0.007451185956597328\n",
      "eval----------\n",
      "regret 0.22959771752357483\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 174\n",
      "loss 0.007409055717289448\n",
      "eval----------\n",
      "regret 0.24866095185279846\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 175\n",
      "loss 0.007435389794409275\n",
      "eval----------\n",
      "regret 0.2564464211463928\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 176\n",
      "loss 0.007451684679836035\n",
      "eval----------\n",
      "regret 0.23138193786144257\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 177\n",
      "loss 0.007444281131029129\n",
      "eval----------\n",
      "regret 0.2594691216945648\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 178\n",
      "loss 0.007415574509650469\n",
      "eval----------\n",
      "regret 0.2512195110321045\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 179\n",
      "loss 0.00741600152105093\n",
      "eval----------\n",
      "regret 0.2244853526353836\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 180\n",
      "loss 0.0074410163797438145\n",
      "eval----------\n",
      "regret 0.2477641999721527\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 181\n",
      "loss 0.007438754662871361\n",
      "eval----------\n",
      "regret 0.2556919455528259\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 182\n",
      "loss 0.0074070212431252\n",
      "eval----------\n",
      "regret 0.23093660175800323\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 183\n",
      "loss 0.0074128806591033936\n",
      "eval----------\n",
      "regret 0.2594647705554962\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 184\n",
      "loss 0.007410152815282345\n",
      "eval----------\n",
      "regret 0.2501707673072815\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 185\n",
      "loss 0.007396641653031111\n",
      "eval----------\n",
      "regret 0.22536343336105347\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 186\n",
      "loss 0.0073973857797682285\n",
      "eval----------\n",
      "regret 0.24778425693511963\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 187\n",
      "loss 0.0074163940735161304\n",
      "eval----------\n",
      "regret 0.25569286942481995\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 188\n",
      "loss 0.007392738480120897\n",
      "eval----------\n",
      "regret 0.23189933598041534\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 189\n",
      "loss 0.0073899091221392155\n",
      "eval----------\n",
      "regret 0.2594657838344574\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 190\n",
      "loss 0.0074041495099663734\n",
      "eval----------\n",
      "regret 0.2519434988498688\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 191\n",
      "loss 0.007378526031970978\n",
      "eval----------\n",
      "regret 0.2328450083732605\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 192\n",
      "loss 0.007375793065875769\n",
      "eval----------\n",
      "regret 0.24834278225898743\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 193\n",
      "loss 0.007386458106338978\n",
      "eval----------\n",
      "regret 0.2574305832386017\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 194\n",
      "loss 0.007385659031569958\n",
      "eval----------\n",
      "regret 0.23645536601543427\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 195\n",
      "loss 0.007394223939627409\n",
      "eval----------\n",
      "regret 0.2659657597541809\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 196\n",
      "loss 0.007375194691121578\n",
      "eval----------\n",
      "regret 0.2640000879764557\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 197\n",
      "loss 0.007396747823804617\n",
      "eval----------\n",
      "regret 0.23658695816993713\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 198\n",
      "loss 0.007350825238972902\n",
      "eval----------\n",
      "regret 0.24712276458740234\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 199\n",
      "loss 0.00736062740907073\n",
      "eval----------\n",
      "regret 0.2564021050930023\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 200\n",
      "loss 0.007344851735979319\n",
      "eval----------\n",
      "regret 0.23360390961170197\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 201\n",
      "loss 0.007374179549515247\n",
      "eval----------\n",
      "regret 0.26615357398986816\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 202\n",
      "loss 0.00735823530703783\n",
      "eval----------\n",
      "regret 0.27980929613113403\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 203\n",
      "loss 0.007362633477896452\n",
      "eval----------\n",
      "regret 0.23799127340316772\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 204\n",
      "loss 0.0073715029284358025\n",
      "eval----------\n",
      "regret 0.2517978250980377\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 205\n",
      "loss 0.007367988582700491\n",
      "eval----------\n",
      "regret 0.25849202275276184\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 206\n",
      "loss 0.007347096223384142\n",
      "eval----------\n",
      "regret 0.2302015870809555\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 207\n",
      "loss 0.007349674124270678\n",
      "eval----------\n",
      "regret 0.25946468114852905\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 208\n",
      "loss 0.007347479462623596\n",
      "eval----------\n",
      "regret 0.2560623288154602\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 209\n",
      "loss 0.007343695964664221\n",
      "eval----------\n",
      "regret 0.23177260160446167\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 210\n",
      "loss 0.007335449103266001\n",
      "eval----------\n",
      "regret 0.2655904293060303\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 211\n",
      "loss 0.007337058428674936\n",
      "eval----------\n",
      "regret 0.2943057119846344\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 212\n",
      "loss 0.007324818056076765\n",
      "eval----------\n",
      "regret 0.28246772289276123\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 213\n",
      "loss 0.007331416010856628\n",
      "eval----------\n",
      "regret 0.29145142436027527\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 214\n",
      "loss 0.007338842377066612\n",
      "eval----------\n",
      "regret 0.28992733359336853\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 215\n",
      "loss 0.0073258597403764725\n",
      "eval----------\n",
      "regret 0.2795090973377228\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 216\n",
      "loss 0.007308654952794313\n",
      "eval----------\n",
      "regret 0.2976679801940918\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 217\n",
      "loss 0.00731702009215951\n",
      "eval----------\n",
      "regret 0.3099041283130646\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 218\n",
      "loss 0.007310125511139631\n",
      "eval----------\n",
      "regret 0.2878296375274658\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 219\n",
      "loss 0.007349735591560602\n",
      "eval----------\n",
      "regret 0.2952077090740204\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 220\n",
      "loss 0.00732487952336669\n",
      "eval----------\n",
      "regret 0.2740674316883087\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 221\n",
      "loss 0.0073060402646660805\n",
      "eval----------\n",
      "regret 0.23586486279964447\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 222\n",
      "loss 0.007297228090465069\n",
      "eval----------\n",
      "regret 0.25107142329216003\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 223\n",
      "loss 0.007319394499063492\n",
      "eval----------\n",
      "regret 0.2573472261428833\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 224\n",
      "loss 0.007317261770367622\n",
      "eval----------\n",
      "regret 0.23065133392810822\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 225\n",
      "loss 0.007301665842533112\n",
      "eval----------\n",
      "regret 0.2604353725910187\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 226\n",
      "loss 0.007315753493458033\n",
      "eval----------\n",
      "regret 0.2564273774623871\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 227\n",
      "loss 0.007290172856301069\n",
      "eval----------\n",
      "regret 0.22946985065937042\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 228\n",
      "loss 0.007286076433956623\n",
      "eval----------\n",
      "regret 0.24670886993408203\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 229\n",
      "loss 0.0072897644713521\n",
      "eval----------\n",
      "regret 0.2563816010951996\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 230\n",
      "loss 0.00728596979752183\n",
      "eval----------\n",
      "regret 0.23106180131435394\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 231\n",
      "loss 0.007290756329894066\n",
      "eval----------\n",
      "regret 0.2594640254974365\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 232\n",
      "loss 0.007270779926329851\n",
      "eval----------\n",
      "regret 0.2541278898715973\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 233\n",
      "loss 0.00728187570348382\n",
      "eval----------\n",
      "regret 0.23408785462379456\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 234\n",
      "loss 0.007297821808606386\n",
      "eval----------\n",
      "regret 0.248861163854599\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 235\n",
      "loss 0.007268912624567747\n",
      "eval----------\n",
      "regret 0.2572197914123535\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 236\n",
      "loss 0.007252705283463001\n",
      "eval----------\n",
      "regret 0.23411068320274353\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 237\n",
      "loss 0.0072588264010846615\n",
      "eval----------\n",
      "regret 0.25946781039237976\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 238\n",
      "loss 0.007269687484949827\n",
      "eval----------\n",
      "regret 0.25235190987586975\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 239\n",
      "loss 0.0072726113721728325\n",
      "eval----------\n",
      "regret 0.22536523640155792\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 240\n",
      "loss 0.007275989279150963\n",
      "eval----------\n",
      "regret 0.24716630578041077\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 241\n",
      "loss 0.007276999764144421\n",
      "eval----------\n",
      "regret 0.2550681233406067\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 242\n",
      "loss 0.007265605032444\n",
      "eval----------\n",
      "regret 0.2310658097267151\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 243\n",
      "loss 0.007278463337570429\n",
      "eval----------\n",
      "regret 0.25752249360084534\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 244\n",
      "loss 0.007241467945277691\n",
      "eval----------\n",
      "regret 0.24988487362861633\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 245\n",
      "loss 0.007269563153386116\n",
      "eval----------\n",
      "regret 0.22286026179790497\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 246\n",
      "loss 0.00723990797996521\n",
      "eval----------\n",
      "regret 0.24717263877391815\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 247\n",
      "loss 0.007238876540213823\n",
      "eval----------\n",
      "regret 0.25399163365364075\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 248\n",
      "loss 0.007272092159837484\n",
      "eval----------\n",
      "regret 0.22653499245643616\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 249\n",
      "loss 0.00726589560508728\n",
      "eval----------\n",
      "regret 0.2594686448574066\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 250\n",
      "loss 0.00723497848957777\n",
      "eval----------\n",
      "regret 0.24792173504829407\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 251\n",
      "loss 0.007229585666209459\n",
      "eval----------\n",
      "regret 0.22383040189743042\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 252\n",
      "loss 0.007240365259349346\n",
      "eval----------\n",
      "regret 0.24179653823375702\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 253\n",
      "loss 0.007224442902952433\n",
      "eval----------\n",
      "regret 0.25260889530181885\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 254\n",
      "loss 0.007236579433083534\n",
      "eval----------\n",
      "regret 0.2250097692012787\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 255\n",
      "loss 0.007227047812193632\n",
      "eval----------\n",
      "regret 0.25132787227630615\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 256\n",
      "loss 0.0072123948484659195\n",
      "eval----------\n",
      "regret 0.2434310019016266\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 257\n",
      "loss 0.007211913354694843\n",
      "eval----------\n",
      "regret 0.21761532127857208\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 258\n",
      "loss 0.007227569352835417\n",
      "eval----------\n",
      "regret 0.2154139280319214\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 259\n",
      "loss 0.0072056204080581665\n",
      "eval----------\n",
      "regret 0.24708209931850433\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 260\n",
      "loss 0.007193217519670725\n",
      "eval----------\n",
      "regret 0.22564785182476044\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 261\n",
      "loss 0.007210213225334883\n",
      "eval----------\n",
      "regret 0.2566307485103607\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 262\n",
      "loss 0.007206734269857407\n",
      "eval----------\n",
      "regret 0.24695366621017456\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 263\n",
      "loss 0.00720628397539258\n",
      "eval----------\n",
      "regret 0.22292761504650116\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 264\n",
      "loss 0.0072133466601371765\n",
      "eval----------\n",
      "regret 0.2454177886247635\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 265\n",
      "loss 0.007227103225886822\n",
      "eval----------\n",
      "regret 0.2517656981945038\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 266\n",
      "loss 0.007208972238004208\n",
      "eval----------\n",
      "regret 0.22589454054832458\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 267\n",
      "loss 0.007232959382236004\n",
      "eval----------\n",
      "regret 0.25721269845962524\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 268\n",
      "loss 0.007188279181718826\n",
      "eval----------\n",
      "regret 0.2497844249010086\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 269\n",
      "loss 0.007205302827060223\n",
      "eval----------\n",
      "regret 0.2220451533794403\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 270\n",
      "loss 0.0071958634071052074\n",
      "eval----------\n",
      "regret 0.2450379580259323\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 271\n",
      "loss 0.007189905736595392\n",
      "eval----------\n",
      "regret 0.2522713541984558\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 272\n",
      "loss 0.0071859764866530895\n",
      "eval----------\n",
      "regret 0.22758758068084717\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 273\n",
      "loss 0.007206508424133062\n",
      "eval----------\n",
      "regret 0.25569939613342285\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 274\n",
      "loss 0.007179693318903446\n",
      "eval----------\n",
      "regret 0.24943235516548157\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 275\n",
      "loss 0.007164719980210066\n",
      "eval----------\n",
      "regret 0.22204342484474182\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 276\n",
      "loss 0.007170787546783686\n",
      "eval----------\n",
      "regret 0.24503356218338013\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 277\n",
      "loss 0.007187322247773409\n",
      "eval----------\n",
      "regret 0.24946235120296478\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 278\n",
      "loss 0.007158248219639063\n",
      "eval----------\n",
      "regret 0.22446836531162262\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 279\n",
      "loss 0.007162319961935282\n",
      "eval----------\n",
      "regret 0.2422732561826706\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 280\n",
      "loss 0.007169583812355995\n",
      "eval----------\n",
      "regret 0.2304767221212387\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 281\n",
      "loss 0.007162462454289198\n",
      "eval----------\n",
      "regret 0.16354456543922424\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 282\n",
      "loss 0.007168675772845745\n",
      "eval----------\n",
      "regret 0.1676121950149536\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 283\n",
      "loss 0.007143031340092421\n",
      "eval----------\n",
      "regret 0.15490104258060455\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 284\n",
      "loss 0.007134394720196724\n",
      "eval----------\n",
      "regret 0.1572643220424652\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 285\n",
      "loss 0.007137977052479982\n",
      "eval----------\n",
      "regret 0.17212031781673431\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 286\n",
      "loss 0.00715302862226963\n",
      "eval----------\n",
      "regret 0.22737067937850952\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 287\n",
      "loss 0.007148061413317919\n",
      "eval----------\n",
      "regret 0.19274090230464935\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 288\n",
      "loss 0.007117317058146\n",
      "eval----------\n",
      "regret 0.2002236694097519\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 289\n",
      "loss 0.00713834073394537\n",
      "eval----------\n",
      "regret 0.18282897770404816\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 290\n",
      "loss 0.007137348409742117\n",
      "eval----------\n",
      "regret 0.15678368508815765\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 291\n",
      "loss 0.007172526326030493\n",
      "eval----------\n",
      "regret 0.19008192420005798\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 292\n",
      "loss 0.007131064310669899\n",
      "eval----------\n",
      "regret 0.18304824829101562\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 293\n",
      "loss 0.007132560946047306\n",
      "eval----------\n",
      "regret 0.19968408346176147\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 294\n",
      "loss 0.007100665010511875\n",
      "eval----------\n",
      "regret 0.2213870882987976\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 295\n",
      "loss 0.007141483016312122\n",
      "eval----------\n",
      "regret 0.2206036001443863\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 296\n",
      "loss 0.007135943975299597\n",
      "eval----------\n",
      "regret 0.22316348552703857\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 297\n",
      "loss 0.00712941586971283\n",
      "eval----------\n",
      "regret 0.24532978236675262\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 298\n",
      "loss 0.0071160816587507725\n",
      "eval----------\n",
      "regret 0.18658608198165894\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 299\n",
      "loss 0.007100915070623159\n",
      "eval----------\n",
      "regret 0.15999817848205566\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 300\n",
      "loss 0.007108773570507765\n",
      "eval----------\n",
      "regret 0.1624710112810135\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 301\n",
      "loss 0.007131986785680056\n",
      "eval----------\n",
      "regret 0.15946900844573975\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 302\n",
      "loss 0.0071118674241006374\n",
      "eval----------\n",
      "regret 0.17304545640945435\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 303\n",
      "loss 0.007111305370926857\n",
      "eval----------\n",
      "regret 0.17818115651607513\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 304\n",
      "loss 0.007111887913197279\n",
      "eval----------\n",
      "regret 0.1906546652317047\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 305\n",
      "loss 0.007108296733349562\n",
      "eval----------\n",
      "regret 0.15462754666805267\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 306\n",
      "loss 0.007089316379278898\n",
      "eval----------\n",
      "regret 0.16143980622291565\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 307\n",
      "loss 0.007105533964931965\n",
      "eval----------\n",
      "regret 0.15381263196468353\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 308\n",
      "loss 0.007082220166921616\n",
      "eval----------\n",
      "regret 0.15548628568649292\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 309\n",
      "loss 0.007065733894705772\n",
      "eval----------\n",
      "regret 0.16271211206912994\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 310\n",
      "loss 0.00710243359208107\n",
      "eval----------\n",
      "regret 0.1669328361749649\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 311\n",
      "loss 0.007090105675160885\n",
      "eval----------\n",
      "regret 0.15772420167922974\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 312\n",
      "loss 0.0070852553471922874\n",
      "eval----------\n",
      "regret 0.16257816553115845\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 313\n",
      "loss 0.007096178364008665\n",
      "eval----------\n",
      "regret 0.15776963531970978\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 314\n",
      "loss 0.007093192543834448\n",
      "eval----------\n",
      "regret 0.1568593829870224\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 315\n",
      "loss 0.007087459787726402\n",
      "eval----------\n",
      "regret 0.16838005185127258\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 316\n",
      "loss 0.007071560248732567\n",
      "eval----------\n",
      "regret 0.16724811494350433\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 317\n",
      "loss 0.007079896982759237\n",
      "eval----------\n",
      "regret 0.1523473858833313\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 318\n",
      "loss 0.007053019013255835\n",
      "eval----------\n",
      "regret 0.1624666005373001\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 319\n",
      "loss 0.0070793889462947845\n",
      "eval----------\n",
      "regret 0.15292291343212128\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 320\n",
      "loss 0.007067193277180195\n",
      "eval----------\n",
      "regret 0.15243251621723175\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 321\n",
      "loss 0.0070767272263765335\n",
      "eval----------\n",
      "regret 0.16026435792446136\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 322\n",
      "loss 0.0070551116950809956\n",
      "eval----------\n",
      "regret 0.16133545339107513\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 323\n",
      "loss 0.007049995474517345\n",
      "eval----------\n",
      "regret 0.1500825732946396\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 324\n",
      "loss 0.007055818568915129\n",
      "eval----------\n",
      "regret 0.1608167141675949\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 325\n",
      "loss 0.007069419138133526\n",
      "eval----------\n",
      "regret 0.1529284119606018\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 326\n",
      "loss 0.007047660648822784\n",
      "eval----------\n",
      "regret 0.1540190577507019\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 327\n",
      "loss 0.007054565474390984\n",
      "eval----------\n",
      "regret 0.16287212073802948\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 328\n",
      "loss 0.0070489319041371346\n",
      "eval----------\n",
      "regret 0.1627473384141922\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 329\n",
      "loss 0.0070718820206820965\n",
      "eval----------\n",
      "regret 0.15235461294651031\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 330\n",
      "loss 0.007058122660964727\n",
      "eval----------\n",
      "regret 0.1608172059059143\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 331\n",
      "loss 0.0070422920398414135\n",
      "eval----------\n",
      "regret 0.15292787551879883\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 332\n",
      "loss 0.007010020315647125\n",
      "eval----------\n",
      "regret 0.15126962959766388\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 333\n",
      "loss 0.007023105397820473\n",
      "eval----------\n",
      "regret 0.16140508651733398\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 334\n",
      "loss 0.007030220702290535\n",
      "eval----------\n",
      "regret 0.16133755445480347\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 335\n",
      "loss 0.00703426543623209\n",
      "eval----------\n",
      "regret 0.1523524820804596\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 336\n",
      "loss 0.007009884342551231\n",
      "eval----------\n",
      "regret 0.16081775724887848\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 337\n",
      "loss 0.007035562302917242\n",
      "eval----------\n",
      "regret 0.1537303924560547\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 338\n",
      "loss 0.007023692596703768\n",
      "eval----------\n",
      "regret 0.1540611982345581\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 339\n",
      "loss 0.007048842031508684\n",
      "eval----------\n",
      "regret 0.16140493750572205\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 340\n",
      "loss 0.007026704493910074\n",
      "eval----------\n",
      "regret 0.16351217031478882\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 341\n",
      "loss 0.0070358398370444775\n",
      "eval----------\n",
      "regret 0.15033811330795288\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 342\n",
      "loss 0.006997568532824516\n",
      "eval----------\n",
      "regret 0.15963807702064514\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 343\n",
      "loss 0.00700647197663784\n",
      "eval----------\n",
      "regret 0.15292206406593323\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 344\n",
      "loss 0.007012349087744951\n",
      "eval----------\n",
      "regret 0.1505957394838333\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 345\n",
      "loss 0.007005193270742893\n",
      "eval----------\n",
      "regret 0.1602068841457367\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 346\n",
      "loss 0.0070058186538517475\n",
      "eval----------\n",
      "regret 0.15904682874679565\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 347\n",
      "loss 0.006992058362811804\n",
      "eval----------\n",
      "regret 0.14953714609146118\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 348\n",
      "loss 0.006988586392253637\n",
      "eval----------\n",
      "regret 0.15757101774215698\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 349\n",
      "loss 0.007008962798863649\n",
      "eval----------\n",
      "regret 0.15222293138504028\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 350\n",
      "loss 0.0069742631167173386\n",
      "eval----------\n",
      "regret 0.1505773663520813\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 351\n",
      "loss 0.006980345584452152\n",
      "eval----------\n",
      "regret 0.16140758991241455\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 352\n",
      "loss 0.006997694727033377\n",
      "eval----------\n",
      "regret 0.1575230360031128\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 353\n",
      "loss 0.00699003366753459\n",
      "eval----------\n",
      "regret 0.14916297793388367\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 354\n",
      "loss 0.007008067797869444\n",
      "eval----------\n",
      "regret 0.15826207399368286\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 355\n",
      "loss 0.006993821822106838\n",
      "eval----------\n",
      "regret 0.15292276442050934\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 356\n",
      "loss 0.006970001384615898\n",
      "eval----------\n",
      "regret 0.1505814790725708\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 357\n",
      "loss 0.00696942675858736\n",
      "eval----------\n",
      "regret 0.16009242832660675\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 358\n",
      "loss 0.006990463938564062\n",
      "eval----------\n",
      "regret 0.1575397551059723\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 359\n",
      "loss 0.006967686582356691\n",
      "eval----------\n",
      "regret 0.15008330345153809\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 360\n",
      "loss 0.006950161885470152\n",
      "eval----------\n",
      "regret 0.15825854241847992\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 361\n",
      "loss 0.00696903932839632\n",
      "eval----------\n",
      "regret 0.1515888273715973\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 362\n",
      "loss 0.006957952864468098\n",
      "eval----------\n",
      "regret 0.15057970583438873\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 363\n",
      "loss 0.0069897230714559555\n",
      "eval----------\n",
      "regret 0.15691451728343964\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 364\n",
      "loss 0.006979532539844513\n",
      "eval----------\n",
      "regret 0.15672656893730164\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 365\n",
      "loss 0.00698828836902976\n",
      "eval----------\n",
      "regret 0.15007679164409637\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 366\n",
      "loss 0.00693355780094862\n",
      "eval----------\n",
      "regret 0.15682625770568848\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 367\n",
      "loss 0.0069634816609323025\n",
      "eval----------\n",
      "regret 0.15159207582473755\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 368\n",
      "loss 0.006952963769435883\n",
      "eval----------\n",
      "regret 0.15057706832885742\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 369\n",
      "loss 0.006947633810341358\n",
      "eval----------\n",
      "regret 0.15748371183872223\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 370\n",
      "loss 0.006931931711733341\n",
      "eval----------\n",
      "regret 0.15836448967456818\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 371\n",
      "loss 0.006922905333340168\n",
      "eval----------\n",
      "regret 0.14946958422660828\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 372\n",
      "loss 0.0069628553465008736\n",
      "eval----------\n",
      "regret 0.15682947635650635\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 373\n",
      "loss 0.006955848075449467\n",
      "eval----------\n",
      "regret 0.14579428732395172\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 374\n",
      "loss 0.0069349659606814384\n",
      "eval----------\n",
      "regret 0.1446796953678131\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 375\n",
      "loss 0.006934863980859518\n",
      "eval----------\n",
      "regret 0.10657940059900284\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 376\n",
      "loss 0.00692042987793684\n",
      "eval----------\n",
      "regret 0.1561913788318634\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 377\n",
      "loss 0.006916224490851164\n",
      "eval----------\n",
      "regret 0.13643111288547516\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 378\n",
      "loss 0.006926441565155983\n",
      "eval----------\n",
      "regret 0.09767051786184311\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 379\n",
      "loss 0.006932207383215427\n",
      "eval----------\n",
      "regret 0.1038220226764679\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 380\n",
      "loss 0.006898020394146442\n",
      "eval----------\n",
      "regret 0.10051561146974564\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 381\n",
      "loss 0.006907992996275425\n",
      "eval----------\n",
      "regret 0.07750803977251053\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 382\n",
      "loss 0.006906937807798386\n",
      "eval----------\n",
      "regret 0.09454815089702606\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 383\n",
      "loss 0.006914627738296986\n",
      "eval----------\n",
      "regret 0.08404961228370667\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 384\n",
      "loss 0.006900791544467211\n",
      "eval----------\n",
      "regret 0.11000362038612366\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 385\n",
      "loss 0.006920564454048872\n",
      "eval----------\n",
      "regret 0.07907992601394653\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 386\n",
      "loss 0.006906640250235796\n",
      "eval----------\n",
      "regret 0.0876639187335968\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 387\n",
      "loss 0.006955227814614773\n",
      "eval----------\n",
      "regret 0.106732577085495\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 388\n",
      "loss 0.006882946472615004\n",
      "eval----------\n",
      "regret 0.1556694209575653\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 389\n",
      "loss 0.006923312786966562\n",
      "eval----------\n",
      "regret 0.14806266129016876\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 390\n",
      "loss 0.006879113614559174\n",
      "eval----------\n",
      "regret 0.15684004127979279\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 391\n",
      "loss 0.006899341940879822\n",
      "eval----------\n",
      "regret 0.1506299525499344\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 392\n",
      "loss 0.006903494242578745\n",
      "eval----------\n",
      "regret 0.15156008303165436\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 393\n",
      "loss 0.0068999179638922215\n",
      "eval----------\n",
      "regret 0.15806865692138672\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 394\n",
      "loss 0.006883028894662857\n",
      "eval----------\n",
      "regret 0.15987882018089294\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 395\n",
      "loss 0.006884115748107433\n",
      "eval----------\n",
      "regret 0.1483401358127594\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 396\n",
      "loss 0.006883669178932905\n",
      "eval----------\n",
      "regret 0.1594053953886032\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 397\n",
      "loss 0.0068875085562467575\n",
      "eval----------\n",
      "regret 0.1523018181324005\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 398\n",
      "loss 0.00687991501763463\n",
      "eval----------\n",
      "regret 0.1520116776227951\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 399\n",
      "loss 0.006883244030177593\n",
      "eval----------\n",
      "regret 0.15692758560180664\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 400\n",
      "loss 0.006864937487989664\n",
      "eval----------\n",
      "regret 0.15838481485843658\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 401\n",
      "loss 0.006868177093565464\n",
      "eval----------\n",
      "regret 0.14630575478076935\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 402\n",
      "loss 0.006874131504446268\n",
      "eval----------\n",
      "regret 0.1501845419406891\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 403\n",
      "loss 0.0068659004755318165\n",
      "eval----------\n",
      "regret 0.1431456357240677\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 404\n",
      "loss 0.006848973222076893\n",
      "eval----------\n",
      "regret 0.0975898951292038\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 405\n",
      "loss 0.006847045850008726\n",
      "eval----------\n",
      "regret 0.10587958991527557\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 406\n",
      "loss 0.006847178563475609\n",
      "eval----------\n",
      "regret 0.14964009821414948\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 407\n",
      "loss 0.006864088587462902\n",
      "eval----------\n",
      "regret 0.10953550040721893\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 408\n",
      "loss 0.006832328625023365\n",
      "eval----------\n",
      "regret 0.12336573004722595\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 409\n",
      "loss 0.006849059835076332\n",
      "eval----------\n",
      "regret 0.1138296127319336\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 410\n",
      "loss 0.006841624155640602\n",
      "eval----------\n",
      "regret 0.13876593112945557\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 411\n",
      "loss 0.006877422798424959\n",
      "eval----------\n",
      "regret 0.15243874490261078\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 412\n",
      "loss 0.006837459281086922\n",
      "eval----------\n",
      "regret 0.11592041701078415\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 413\n",
      "loss 0.006854269653558731\n",
      "eval----------\n",
      "regret 0.10334933549165726\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 414\n",
      "loss 0.006818507798016071\n",
      "eval----------\n",
      "regret 0.11911524832248688\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 415\n",
      "loss 0.006842069793492556\n",
      "eval----------\n",
      "regret 0.1131182461977005\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 416\n",
      "loss 0.006833886727690697\n",
      "eval----------\n",
      "regret 0.12480157613754272\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 417\n",
      "loss 0.0068314517848193645\n",
      "eval----------\n",
      "regret 0.129252627491951\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 418\n",
      "loss 0.006821590010076761\n",
      "eval----------\n",
      "regret 0.1556723713874817\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 419\n",
      "loss 0.006827895995229483\n",
      "eval----------\n",
      "regret 0.13612870872020721\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 420\n",
      "loss 0.006831396371126175\n",
      "eval----------\n",
      "regret 0.14262418448925018\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 421\n",
      "loss 0.006829078309237957\n",
      "eval----------\n",
      "regret 0.10301515460014343\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 422\n",
      "loss 0.006811683531850576\n",
      "eval----------\n",
      "regret 0.1396765261888504\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 423\n",
      "loss 0.006817960180342197\n",
      "eval----------\n",
      "regret 0.10410962998867035\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 424\n",
      "loss 0.0068181655369699\n",
      "eval----------\n",
      "regret 0.08645293116569519\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 425\n",
      "loss 0.006824288051575422\n",
      "eval----------\n",
      "regret 0.07801836729049683\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 426\n",
      "loss 0.006829225458204746\n",
      "eval----------\n",
      "regret 0.07508222758769989\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 427\n",
      "loss 0.0068003274500370026\n",
      "eval----------\n",
      "regret 0.06886227428913116\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 428\n",
      "loss 0.006779783871024847\n",
      "eval----------\n",
      "regret 0.07566527277231216\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 429\n",
      "loss 0.00679683405905962\n",
      "eval----------\n",
      "regret 0.06750352680683136\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 430\n",
      "loss 0.006825095042586327\n",
      "eval----------\n",
      "regret 0.07404370605945587\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 431\n",
      "loss 0.006792198400944471\n",
      "eval----------\n",
      "regret 0.07691445201635361\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 432\n",
      "loss 0.006796465255320072\n",
      "eval----------\n",
      "regret 0.07866620272397995\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 433\n",
      "loss 0.006803667638450861\n",
      "eval----------\n",
      "regret 0.10939004272222519\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 434\n",
      "loss 0.006799294147640467\n",
      "eval----------\n",
      "regret 0.11886704713106155\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 435\n",
      "loss 0.006825033109635115\n",
      "eval----------\n",
      "regret 0.15187868475914001\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 436\n",
      "loss 0.006789179984480143\n",
      "eval----------\n",
      "regret 0.1567354053258896\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 437\n",
      "loss 0.006813017185777426\n",
      "eval----------\n",
      "regret 0.14951808750629425\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 438\n",
      "loss 0.00677382992580533\n",
      "eval----------\n",
      "regret 0.1557028591632843\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 439\n",
      "loss 0.006775351241230965\n",
      "eval----------\n",
      "regret 0.1502612829208374\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 440\n",
      "loss 0.0067884065210819244\n",
      "eval----------\n",
      "regret 0.15058337152004242\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 441\n",
      "loss 0.0067785014398396015\n",
      "eval----------\n",
      "regret 0.1559344232082367\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 442\n",
      "loss 0.006770438514649868\n",
      "eval----------\n",
      "regret 0.1561909317970276\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 443\n",
      "loss 0.00677198963239789\n",
      "eval----------\n",
      "regret 0.1472741663455963\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 444\n",
      "loss 0.006766043603420258\n",
      "eval----------\n",
      "regret 0.1544302999973297\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 445\n",
      "loss 0.006769106257706881\n",
      "eval----------\n",
      "regret 0.12604431807994843\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 446\n",
      "loss 0.006761463358998299\n",
      "eval----------\n",
      "regret 0.13731688261032104\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 447\n",
      "loss 0.006763889454305172\n",
      "eval----------\n",
      "regret 0.11403496563434601\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 448\n",
      "loss 0.0067746639251708984\n",
      "eval----------\n",
      "regret 0.14635103940963745\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 449\n",
      "loss 0.006760385353118181\n",
      "eval----------\n",
      "regret 0.08356118947267532\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 450\n",
      "loss 0.00676459725946188\n",
      "eval----------\n",
      "regret 0.07395351678133011\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 451\n",
      "loss 0.006731026340276003\n",
      "eval----------\n",
      "regret 0.07729879766702652\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 452\n",
      "loss 0.006733998190611601\n",
      "eval----------\n",
      "regret 0.08839267492294312\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 453\n",
      "loss 0.00673256628215313\n",
      "eval----------\n",
      "regret 0.07566691935062408\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 454\n",
      "loss 0.00673890532925725\n",
      "eval----------\n",
      "regret 0.0817253366112709\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 455\n",
      "loss 0.006748447194695473\n",
      "eval----------\n",
      "regret 0.0769166648387909\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 456\n",
      "loss 0.006739041768014431\n",
      "eval----------\n",
      "regret 0.07584397494792938\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 457\n",
      "loss 0.0067468979395926\n",
      "eval----------\n",
      "regret 0.071216881275177\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 458\n",
      "loss 0.0067384932190179825\n",
      "eval----------\n",
      "regret 0.07806837558746338\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 459\n",
      "loss 0.006784982979297638\n",
      "eval----------\n",
      "regret 0.07425786554813385\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 460\n",
      "loss 0.006714717019349337\n",
      "eval----------\n",
      "regret 0.09347829967737198\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 461\n",
      "loss 0.006755763664841652\n",
      "eval----------\n",
      "regret 0.0921795666217804\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 462\n",
      "loss 0.006701173260807991\n",
      "eval----------\n",
      "regret 0.07805348187685013\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 463\n",
      "loss 0.006734326481819153\n",
      "eval----------\n",
      "regret 0.09824741631746292\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 464\n",
      "loss 0.006735042668879032\n",
      "eval----------\n",
      "regret 0.10912097245454788\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 465\n",
      "loss 0.0067222886718809605\n",
      "eval----------\n",
      "regret 0.0852401852607727\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 466\n",
      "loss 0.006713733542710543\n",
      "eval----------\n",
      "regret 0.08118415623903275\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 467\n",
      "loss 0.006696448661386967\n",
      "eval----------\n",
      "regret 0.07691796123981476\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 468\n",
      "loss 0.006710503716021776\n",
      "eval----------\n",
      "regret 0.07233406603336334\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 469\n",
      "loss 0.006723322439938784\n",
      "eval----------\n",
      "regret 0.06581298261880875\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 470\n",
      "loss 0.006700935307890177\n",
      "eval----------\n",
      "regret 0.0747215524315834\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 471\n",
      "loss 0.006704222876578569\n",
      "eval----------\n",
      "regret 0.06334864348173141\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 472\n",
      "loss 0.006702672690153122\n",
      "eval----------\n",
      "regret 0.07535596191883087\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 473\n",
      "loss 0.006713961251080036\n",
      "eval----------\n",
      "regret 0.07691357284784317\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 474\n",
      "loss 0.006703169085085392\n",
      "eval----------\n",
      "regret 0.07209557294845581\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 475\n",
      "loss 0.006703543942421675\n",
      "eval----------\n",
      "regret 0.06290145963430405\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 476\n",
      "loss 0.006682709790766239\n",
      "eval----------\n",
      "regret 0.07382979243993759\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 477\n",
      "loss 0.006677551660686731\n",
      "eval----------\n",
      "regret 0.06654825061559677\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 478\n",
      "loss 0.0067000663839280605\n",
      "eval----------\n",
      "regret 0.07603363692760468\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 479\n",
      "loss 0.006672547664493322\n",
      "eval----------\n",
      "regret 0.07690408080816269\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 480\n",
      "loss 0.006688365712761879\n",
      "eval----------\n",
      "regret 0.07507091015577316\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 481\n",
      "loss 0.006708516739308834\n",
      "eval----------\n",
      "regret 0.06606407463550568\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 482\n",
      "loss 0.006701437756419182\n",
      "eval----------\n",
      "regret 0.07477374374866486\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 483\n",
      "loss 0.006708846427500248\n",
      "eval----------\n",
      "regret 0.06334105134010315\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 484\n",
      "loss 0.0067012920044362545\n",
      "eval----------\n",
      "regret 0.07599921524524689\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 485\n",
      "loss 0.006691610906273127\n",
      "eval----------\n",
      "regret 0.07691445201635361\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 486\n",
      "loss 0.00667955307289958\n",
      "eval----------\n",
      "regret 0.07507951557636261\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 487\n",
      "loss 0.006670541130006313\n",
      "eval----------\n",
      "regret 0.07538077980279922\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 488\n",
      "loss 0.006688309833407402\n",
      "eval----------\n",
      "regret 0.07572747021913528\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 489\n",
      "loss 0.006682811304926872\n",
      "eval----------\n",
      "regret 0.06517542898654938\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 490\n",
      "loss 0.006655803881585598\n",
      "eval----------\n",
      "regret 0.07845932990312576\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 491\n",
      "loss 0.006667446345090866\n",
      "eval----------\n",
      "regret 0.07691021263599396\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 492\n",
      "loss 0.006666578818112612\n",
      "eval----------\n",
      "regret 0.07507466524839401\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 493\n",
      "loss 0.0066666011698544025\n",
      "eval----------\n",
      "regret 0.06836746633052826\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 494\n",
      "loss 0.0066701374016702175\n",
      "eval----------\n",
      "regret 0.07506520301103592\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 495\n",
      "loss 0.0066590323112905025\n",
      "eval----------\n",
      "regret 0.06647919118404388\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 496\n",
      "loss 0.006671938579529524\n",
      "eval----------\n",
      "regret 0.07404588907957077\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 497\n",
      "loss 0.006649770773947239\n",
      "eval----------\n",
      "regret 0.0769142284989357\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 498\n",
      "loss 0.006644866894930601\n",
      "eval----------\n",
      "regret 0.07290670275688171\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 499\n",
      "loss 0.006643158383667469\n",
      "eval----------\n",
      "regret 0.06614890694618225\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 500\n",
      "loss 0.006611150689423084\n",
      "eval----------\n",
      "regret 0.07603141665458679\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 501\n",
      "loss 0.006620165891945362\n",
      "eval----------\n",
      "regret 0.06992286443710327\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 502\n",
      "loss 0.0066436356864869595\n",
      "eval----------\n",
      "regret 0.07860717177391052\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 503\n",
      "loss 0.006612321827560663\n",
      "eval----------\n",
      "regret 0.07691358774900436\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 504\n",
      "loss 0.006633316166698933\n",
      "eval----------\n",
      "regret 0.07508257031440735\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 505\n",
      "loss 0.00664201844483614\n",
      "eval----------\n",
      "regret 0.07097694277763367\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 506\n",
      "loss 0.0066225784830749035\n",
      "eval----------\n",
      "regret 0.07383949309587479\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 507\n",
      "loss 0.006656242534518242\n",
      "eval----------\n",
      "regret 0.06981024146080017\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 508\n",
      "loss 0.006621832028031349\n",
      "eval----------\n",
      "regret 0.09541439265012741\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 509\n",
      "loss 0.006653097458183765\n",
      "eval----------\n",
      "regret 0.0975513830780983\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 510\n",
      "loss 0.006606475915759802\n",
      "eval----------\n",
      "regret 0.07756958156824112\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 511\n",
      "loss 0.006622086279094219\n",
      "eval----------\n",
      "regret 0.07428959012031555\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 512\n",
      "loss 0.0066026668064296246\n",
      "eval----------\n",
      "regret 0.07768037170171738\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 513\n",
      "loss 0.006633468437939882\n",
      "eval----------\n",
      "regret 0.06899762898683548\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 514\n",
      "loss 0.006597547791898251\n",
      "eval----------\n",
      "regret 0.07666473090648651\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 515\n",
      "loss 0.0066025941632688046\n",
      "eval----------\n",
      "regret 0.07692302018404007\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 516\n",
      "loss 0.006614190526306629\n",
      "eval----------\n",
      "regret 0.07407576590776443\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 517\n",
      "loss 0.006623389199376106\n",
      "eval----------\n",
      "regret 0.0657612532377243\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 518\n",
      "loss 0.0066071124747395515\n",
      "eval----------\n",
      "regret 0.07384555041790009\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 519\n",
      "loss 0.00661845039576292\n",
      "eval----------\n",
      "regret 0.06543610990047455\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 520\n",
      "loss 0.0065806047059595585\n",
      "eval----------\n",
      "regret 0.07478097081184387\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 521\n",
      "loss 0.0065976898185908794\n",
      "eval----------\n",
      "regret 0.07692106068134308\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 522\n",
      "loss 0.006585986819118261\n",
      "eval----------\n",
      "regret 0.07190524786710739\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 523\n",
      "loss 0.006595627870410681\n",
      "eval----------\n",
      "regret 0.06779489666223526\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 524\n",
      "loss 0.006555094849318266\n",
      "eval----------\n",
      "regret 0.07384441047906876\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 525\n",
      "loss 0.006593312136828899\n",
      "eval----------\n",
      "regret 0.06754124909639359\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 526\n",
      "loss 0.006594937294721603\n",
      "eval----------\n",
      "regret 0.07868544012308121\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 527\n",
      "loss 0.006569332908838987\n",
      "eval----------\n",
      "regret 0.07819165289402008\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 528\n",
      "loss 0.00658887904137373\n",
      "eval----------\n",
      "regret 0.07467381656169891\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 529\n",
      "loss 0.006598158739507198\n",
      "eval----------\n",
      "regret 0.07303629070520401\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 530\n",
      "loss 0.0065655577927827835\n",
      "eval----------\n",
      "regret 0.07709627598524094\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 531\n",
      "loss 0.006618776358664036\n",
      "eval----------\n",
      "regret 0.07044454663991928\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 532\n",
      "loss 0.006585432682186365\n",
      "eval----------\n",
      "regret 0.07532324641942978\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 533\n",
      "loss 0.006606245879083872\n",
      "eval----------\n",
      "regret 0.07691387832164764\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 534\n",
      "loss 0.0065471213310956955\n",
      "eval----------\n",
      "regret 0.07067451626062393\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 535\n",
      "loss 0.006558738183230162\n",
      "eval----------\n",
      "regret 0.06520169973373413\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 536\n",
      "loss 0.006566735450178385\n",
      "eval----------\n",
      "regret 0.07383712381124496\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 537\n",
      "loss 0.0065724668093025684\n",
      "eval----------\n",
      "regret 0.0628277063369751\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 538\n",
      "loss 0.00654893834143877\n",
      "eval----------\n",
      "regret 0.07404828816652298\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 539\n",
      "loss 0.006539176218211651\n",
      "eval----------\n",
      "regret 0.07691479474306107\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 540\n",
      "loss 0.006549341604113579\n",
      "eval----------\n",
      "regret 0.07065597176551819\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 541\n",
      "loss 0.006569793913513422\n",
      "eval----------\n",
      "regret 0.06290990114212036\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 542\n",
      "loss 0.0065711564384400845\n",
      "eval----------\n",
      "regret 0.07289034128189087\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 543\n",
      "loss 0.00654483400285244\n",
      "eval----------\n",
      "regret 0.06238855794072151\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 544\n",
      "loss 0.0065485951490700245\n",
      "eval----------\n",
      "regret 0.07405311614274979\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 545\n",
      "loss 0.0065507907420396805\n",
      "eval----------\n",
      "regret 0.07692292332649231\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 546\n",
      "loss 0.006553311832249165\n",
      "eval----------\n",
      "regret 0.07117416709661484\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 547\n",
      "loss 0.006546811666339636\n",
      "eval----------\n",
      "regret 0.0629081204533577\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 548\n",
      "loss 0.006517535075545311\n",
      "eval----------\n",
      "regret 0.0719764456152916\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 549\n",
      "loss 0.006535726599395275\n",
      "eval----------\n",
      "regret 0.062388524413108826\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 550\n",
      "loss 0.006527503486722708\n",
      "eval----------\n",
      "regret 0.07405071705579758\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 551\n",
      "loss 0.006532746367156506\n",
      "eval----------\n",
      "regret 0.07691638171672821\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 552\n",
      "loss 0.006531646940857172\n",
      "eval----------\n",
      "regret 0.07062423974275589\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 553\n",
      "loss 0.00653882697224617\n",
      "eval----------\n",
      "regret 0.0629076138138771\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 554\n",
      "loss 0.0065143173560500145\n",
      "eval----------\n",
      "regret 0.07197502255439758\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 555\n",
      "loss 0.00657280907034874\n",
      "eval----------\n",
      "regret 0.062381353229284286\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 556\n",
      "loss 0.006541374605149031\n",
      "eval----------\n",
      "regret 0.07404777407646179\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 557\n",
      "loss 0.006543279625475407\n",
      "eval----------\n",
      "regret 0.07691218703985214\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 558\n",
      "loss 0.006512651219964027\n",
      "eval----------\n",
      "regret 0.07016207277774811\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 559\n",
      "loss 0.006516342982649803\n",
      "eval----------\n",
      "regret 0.06290372461080551\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 560\n",
      "loss 0.006517671048641205\n",
      "eval----------\n",
      "regret 0.07294439524412155\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "\n",
      " UPDATE \n",
      "\n",
      "------------------------------------iteration: 561\n",
      "loss 0.00650444021448493\n",
      "eval----------\n",
      "regret 0.06238151714205742\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 562\n",
      "loss 0.006507247220724821\n",
      "eval----------\n",
      "regret 0.07404401153326035\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 563\n",
      "loss 0.006512424908578396\n",
      "eval----------\n",
      "regret 0.07691337913274765\n",
      "eval done------\n",
      " train loss: 0.007, valid loss: 0.007 \n",
      "------------------------------------iteration: 564\n",
      "loss 0.006528658326715231\n",
      "eval----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yehya\\Desktop\\SP_exp\\two_stage.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m val_loss \u001b[39m=\u001b[39m loss_fn(linear_layer_output_val, val_targets)\u001b[39m# + 0.9*a_v + 0.1*b_v\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m val_losses\u001b[39m.\u001b[39mappend(val_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m regret \u001b[39m=\u001b[39m regret_cvxpy(val_targets, linear_layer_output_val, cvx_py) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mregret\u001b[39m\u001b[39m\"\u001b[39m,regret\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m regret_lst\u001b[39m.\u001b[39mappend(regret\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;32mc:\\Users\\yehya\\Desktop\\SP_exp\\two_stage.ipynb Cell 13\u001b[0m in \u001b[0;36mregret_cvxpy\u001b[1;34m(y_true, y_hat, cvx)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m A \u001b[39m=\u001b[39m inci_matrix    \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m y_true_decision \u001b[39m=\u001b[39m cvx(A,y_true)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m y_hat_decision \u001b[39m=\u001b[39m cvx(A,y_hat)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m objective_true \u001b[39m=\u001b[39m (y_true \u001b[39m*\u001b[39m y_true_decision)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m objective_hat \u001b[39m=\u001b[39m  (y_true \u001b[39m*\u001b[39m y_hat_decision)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\yehya\\Desktop\\SP_exp\\two_stage.ipynb Cell 13\u001b[0m in \u001b[0;36mcvx_py\u001b[1;34m(A, pred)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m pred:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m      \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39m#sol = solutions(x)\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     target[index] \u001b[39m=\u001b[39m solutions(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m#print(problem.value)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     index \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32mc:\\Users\\yehya\\Desktop\\SP_exp\\two_stage.ipynb Cell 13\u001b[0m in \u001b[0;36mcvx_py.<locals>.<lambda>\u001b[1;34m(z)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m problem  \u001b[39m=\u001b[39m cp\u001b[39m.\u001b[39mProblem(cp\u001b[39m.\u001b[39mMinimize(c \u001b[39m@\u001b[39m x ),constr)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m cvxlayer \u001b[39m=\u001b[39m CvxpyLayer(problem, parameters\u001b[39m=\u001b[39m[c], variables\u001b[39m=\u001b[39m[x])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m solutions \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m z: cvxlayer(z)[\u001b[39m0\u001b[39m] \u001b[39m#, args = {'tol':1e-10})[0]\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m index \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m pred:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m      \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yehya/Desktop/SP_exp/two_stage.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39m#sol = solutions(x)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\yehya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\yehya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py:152\u001b[0m, in \u001b[0;36mCvxpyLayer.forward\u001b[1;34m(self, solver_args, *params)\u001b[0m\n\u001b[0;32m    139\u001b[0m info \u001b[39m=\u001b[39m {}\n\u001b[0;32m    140\u001b[0m f \u001b[39m=\u001b[39m _CvxpyLayerFn(\n\u001b[0;32m    141\u001b[0m     param_order\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_order,\n\u001b[0;32m    142\u001b[0m     param_ids\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m     info\u001b[39m=\u001b[39minfo,\n\u001b[0;32m    151\u001b[0m )\n\u001b[1;32m--> 152\u001b[0m sol \u001b[39m=\u001b[39m f(\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m info\n\u001b[0;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m sol\n",
      "File \u001b[1;32mc:\\Users\\yehya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py:309\u001b[0m, in \u001b[0;36m_CvxpyLayerFn.<locals>._CvxpyLayerFnFn.forward\u001b[1;34m(ctx, *params)\u001b[0m\n\u001b[0;32m    306\u001b[0m sol \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mcat(s, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sol]\n\u001b[0;32m    308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ctx\u001b[39m.\u001b[39mbatch:\n\u001b[1;32m--> 309\u001b[0m     sol \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sol]\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m gp:\n\u001b[0;32m    312\u001b[0m     sol \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mexp(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sol]\n",
      "File \u001b[1;32mc:\\Users\\yehya\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cvxpylayers\\torch\\cvxpylayer.py:309\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    306\u001b[0m sol \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mcat(s, \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sol]\n\u001b[0;32m    308\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ctx\u001b[39m.\u001b[39mbatch:\n\u001b[1;32m--> 309\u001b[0m     sol \u001b[39m=\u001b[39m [s\u001b[39m.\u001b[39;49msqueeze(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sol]\n\u001b[0;32m    311\u001b[0m \u001b[39mif\u001b[39;00m gp:\n\u001b[0;32m    312\u001b[0m     sol \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mexp(s) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m sol]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#early_stop_patience = 10\n",
    "#min_val_loss = -10\n",
    "#patience_counter = 0\n",
    "#patience = 10\n",
    "\n",
    "best = 10000.0\n",
    "patience = 20\n",
    "fails = 0\n",
    "flag = False\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "regret_lst = []\n",
    "\n",
    "#l1_error = []\n",
    "\n",
    "#avg_violations = []\n",
    "#max_violations = []\n",
    "\n",
    "#l1_loss_fn = nn.L1Loss()\n",
    "\n",
    "train_iterator = itertools.cycle(train_dl)\n",
    "\n",
    "val_iter = itertools.cycle(val_dl)\n",
    "\n",
    "#flow_viol_lst = []\n",
    "#selection_viol_lst = []\n",
    "\n",
    "\n",
    "\n",
    "for iteration in range(0, params['iterations']):\n",
    "  # loop over the dataset multiple times\n",
    "  \n",
    "  print(\"------------------------------------iteration: {}\".format(iteration))\n",
    "    #running_loss = 0.0\n",
    "   \n",
    "    \n",
    "    \n",
    "  model.train()\n",
    "  \n",
    "  #train_iterator = itertools.cycle(train_dl)\n",
    "  data = next(iter(train_iterator))\n",
    "  inputs, targets  = data\n",
    "  \n",
    " \n",
    "  \n",
    "  \n",
    "  prediction = model(inputs)\n",
    "      \n",
    " \n",
    "  loss = loss_fn(prediction,targets) \n",
    "    \n",
    "    \n",
    "  print(\"loss\", loss.item())  \n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  optimizer.zero_grad()\n",
    "    \n",
    "  train_losses.append(loss.item())\n",
    "  \n",
    "  with torch.no_grad(): \n",
    "    model.eval()\n",
    "    val_data = next(iter(val_iter))\n",
    "    \n",
    "    val_inputs, val_targets = val_data\n",
    "      \n",
    "      #with torch.no_grad():\n",
    "    print(\"eval----------\")\n",
    "    linear_layer_output_val = model(val_inputs)\n",
    "    #max_v_, avg_v_ , a_v, b_v= calc_violations(inci_matrix, linear_layer_output_val.detach())\n",
    "    val_loss = loss_fn(linear_layer_output_val, val_targets)# + 0.9*a_v + 0.1*b_v\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    regret = regret_cvxpy(val_targets, linear_layer_output_val, cvx_py) \n",
    "    print(\"regret\",regret.item())\n",
    "    regret_lst.append(regret.item())\n",
    "    print(\"eval done------\")\n",
    "  \n",
    "  print(' train loss: {}, valid loss: {} '.format(round(loss.item(), 3), round(val_loss.item(),3)))\n",
    "  \n",
    "  if iteration%10==0:\n",
    "    if val_loss < (best - 1e-5):\n",
    "      print(\"\\n UPDATE \\n\")\n",
    "      best_model = copy.deepcopy(model)\n",
    "      fails = 0\n",
    "      best = val_loss\n",
    "    else:\n",
    "      fails = fails + 1\n",
    "    if fails > patience:\n",
    "      print(\"Early Stopping. Valid hasn't improved for {}\".format(patience))\n",
    "      flag = True\n",
    "    if flag:\n",
    "      break\n",
    "    loss = 0\n",
    "    val_loss = 0\n",
    "   \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 weights tensor(0.0032, grad_fn=<DivBackward0>)\n",
      "l2 decision tensor(0.0031, grad_fn=<DivBackward0>)\n",
      "inf time 0.475\n",
      "violations tensor(0.0002, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "test_iter = itertools.cycle(test_dl)\n",
    "test_l2 = nn.MSELoss() \n",
    "#test_l1 = nn.L1Loss() \n",
    "\n",
    "l2_weights = []\n",
    "l2_decision = []\n",
    "test_viol = []\n",
    "optimal_path_acc = []\n",
    "inf_time = []\n",
    "\n",
    "#optimal_path_acc = []\n",
    "\n",
    "i =0 \n",
    "for inputs, targets in test_iter:\n",
    "    \n",
    "    start = time.process_time()\n",
    "    linear_layer_output = torch.nn.functional.normalize(model(inputs))\n",
    "    selected_edges = cvx_py(inci_matrix,linear_layer_output)\n",
    "    inf_time.append(time.process_time() - start)\n",
    "    \n",
    "    error_w = test_l2(linear_layer_output,targets)\n",
    "    l2_weights.append(error_w)\n",
    "    \n",
    "    selected_edges_targets = cvx_py(inci_matrix,targets)\n",
    "    error_d = test_l2(selected_edges,selected_edges_targets)\n",
    "    l2_decision.append(error_d)\n",
    "    \n",
    "    vio = calc_violations(inci_matrix,selected_edges.detach())[1]\n",
    "    test_viol.append(vio)\n",
    "    #selected_edges = model_load(linear_layer_output)\n",
    "    #selected_edges_targets = cvx_py(A,linear_layer_output) \n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break\n",
    "\n",
    "print(\"l2 weights\", sum(l2_weights)/len(l2_weights))\n",
    "print(\"l2 decision\", sum(l2_decision)/len(l2_decision))\n",
    "print(\"inf time\", sum(inf_time)/len(inf_time))\n",
    "print(\"violations\", sum(test_viol)/len(test_viol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses,color='r', label='train loss')\n",
    "plt.plot(val_losses, color='g', label='val loss')\n",
    "\n",
    "plt.xlabel(\"loss\")\n",
    "plt.ylabel(\"iterations\")\n",
    "plt.title(\"loss\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAq40lEQVR4nO3deXxU5b3H8c9vEhL2fZFNA4ggKoKiuIuKihuo1etW17ZWrdVbWy0uVavV2nqv1bZWpdblti51qRUVxaogiooEURTZIjsKBJAdkkzyu3/MSRhigGRmTmaS+b5fr7xytnnOLwdmfvM85znPY+6OiIhkr0i6AxARkfRSIhARyXJKBCIiWU6JQEQkyykRiIhkOSUCEZEsp0QgIpLllAhE0szM3Mz2THcckr2UCESqMbPcTCxLJCxKBCKAmS00s1+a2Qxgk5kdYWYfmNlaM/vMzIbFHdvLzCaZ2QYze8vMHjSzfwT7CoJv+D8ws8XAO8H2y8xslpl9a2bjzWyPYPukoNjPzGyjmZ1Tr3+4CKBvKyLbnAecAlQAM4ALgTeA44AXzay/uxcDTwOTgeHAwcA4YGy1so4G9gYqzGwUcBNwGjAPGA08Axzm7keZmQP7u3tRyH+fSI1MYw2JxGoEwB3u/piZ/RLY190vjNs/nlgCmADMB1q7++Zg3z8A3P37ZlYALAD6uPv8YP/rwAvu/rdgPQJsBPZ290VBIuirRCDpoqYhkW2WBL/3AM4OmoXWmtla4AigK9ANWFOZBKq9rqayKst7IK6sNYAB3VMcv0hC1DQksk1l9XgJ8Hd3/1H1A4K2/fZm1jwuGfTcSVmV5d3l7k+lNFqRFFGNQOS7/gGcZmYnmlmOmTU1s2Fm1sPdFwGFwO1mlmdmhxJr+9+Zh4EbzWwfADNrY2Znx+1fAfQO4w8RqQ0lApFq3H0JUHmDt5jYN/rr2fZ+uQA4FFgN/Ab4J1Cyk/JeAn4HPGtm64EvgJPiDrkdeDJoOvqvlP4xIrWgm8UiSTKzfwKz3f22dMcikgjVCETqyMwOMrM+ZhYxsxHEag//TnNYIgnTzWKRutsN+BfQAVgKXOnu09Mbkkji1DQkIpLl1DQkIpLlGlzTUMeOHb2goCDdYYiINCjTpk1b5e6datrX4BJBQUEBhYWF6Q5DRKRBMbNFO9qnpiERkSynRCAikuWUCEREspwSgYhIllMiEBHJckoEIiJZTolARCTLZU0iiJZX8NzUJZRXaEgNEZF4WZMI/vHRIm54cQZPTdnhMxUiIlkpaxLBms1lAKzeWJrmSEREMkvWJIKIxX5rtFURke1lUSKIZQLdIhAR2V4WJYLY7wrVCEREtpM1icBUIxARqVEWJYLYb0eZQEQkXqiJwMxGmNkcMysys9E17L/EzIrN7NPg54dhxVLZIpRTmRFERAQIcWIaM8sBHgSOJzbB91QzG+vuX1Y79J/ufnVYcVSKlscyQU5EiUBEJF6YNYKDgSJ3n+/upcCzwKgQz7dTVx3TB4DcSNa0homI1EqYn4rdgSVx60uDbdV9z8xmmNkLZtYzrGByg5pAuXoNiYhsJ91fj18BCtx9IPAf4MmaDjKzy82s0MwKi4uLEzqRmRExqFC3IRGR7YSZCJYB8d/wewTbqrj7ancvCVYfBQ6sqSB3H+PuQ9x9SKdOnRIOKDcSUY1ARKSaMBPBVKCvmfUyszzgXGBs/AFm1jVudSQwK8R4iETQ6KMiItWE1mvI3aNmdjUwHsgBHnP3mWZ2B1Do7mOBa8xsJBAF1gCXhBUPxLqOKhGIiGwvtEQA4O7jgHHVtt0at3wjcGOYMcTLiSgRiIhUF2oiyDTrt0aZvXw9M79exz7d2jBn+QYiBk7sgbMtZeUY0Cwvp+o10XJna7Sc/NwIeTkRnFgPpN6dWqbrzxARSamsSgQAH81fwyl/fJ/CW4Zz4v2TEi5n4T2npDAqEZH0SXf30bQpiVakOwQRkYyQtYkg2YEmSqLlKYlDRCTdsjYRlJUnVyPYsDWaokhERNJLiSBB78xemaJIRETSK6sSwf492lQtl0aT60Z6wwszkg1HRCQjZFUiGL53l6rlZGsEIiKNRVYlgvg6QKkSgYgIkGWJIF6Zuo+KiABZlgjiu4yqRiAiEpNViSBeWbnGHBIRgaxOBKoRiIhAlo01dNGhBYx5bz4btka3SwQn77cb0XKnSU6QFw3cnS2l5eREIuTlGhY0LL32+TfpCF1EJDRZlQjaNG/CXy8awrljPmLVxlIAfn/WQP5rSO2nSj7l82+46qlPwgpRRKTeZV3TUE4wif2dr34JQMTqNurQiH12A+CQ3u1TG5iISJpkbSLYtl6310cixuDd25IbybpLJyKNVNZ9muVUqwHUtUYA0DI/l02lGnRORBqH7EsEkdQkgo0afVREGomsSwS5Odt/8CeQB2jVNFfDUItIo5F1iaB601Aik9k3z1PTkIg0HtmXCCLJJ4K83IgeSBORRiPrE0HL/Lo/SpGXE6FUg9aJSCOR9Ylgz84t61xGXm6ECoeoagUi0ghkfSLo3SmxRAAawVREGoesTwSJqByTqCzJ6S5FRDJB1iWCVDwRvCXoMTRv5YakyxIRSbesSwTx3UcnXX9MQmV8sngtAE9PWZyKkERE0ir7EkHwQFleboTdOzRPqIzrT+wHwFANPCcijUCoicDMRpjZHDMrMrPROznue2bmZjYkzHgAKm8RJHOnoFOrfABK1IVURBqB0BKBmeUADwInAQOA88xsQA3HtQKuBaaEFUu8ypvFg3q2TbiM/KDXUEmZEoGINHxhTkxzMFDk7vMBzOxZYBTwZbXj7gR+B1wfYixV8nNzeOmqw+iTwPMDlZo2yQFga1l5qsISEUmbMJuGugNL4taXBtuqmNkBQE93f21nBZnZ5WZWaGaFxcXFSQc2ePd2tG7aJOHX50aMiKlpSEQah7TdLDazCHAf8PNdHevuY9x9iLsP6dSpU/jB7YKZ0bRJDiVR1QhEpOELMxEsA+InA+4RbKvUCtgXmGhmC4FDgLH1ccM4FfJzI2zVPQIRaQTCTARTgb5m1svM8oBzgbGVO919nbt3dPcCdy8APgJGunthiDGlTH6uagQi0jiElgjcPQpcDYwHZgHPuftMM7vDzEaGdd760rSJagQi0jiE2WsIdx8HjKu27dYdHDsszFhSbeHqzSxcvZk/njc43aGIiCQl654sFhGR7SkRiIhkOSWCBP38+L0ANGWliDR4SgQJym8Su3R6ulhEGjolggRVDjNx/fMz0hyJiEhylAgSVF4Rm53sjZnL0xyJiEhylAgS1KZZ4mMViYhkEiWCBJ0+KDZ+3sj9u6U5EhGR5CgRJCgSMXq0a0ZuJJkpbkRE0k+JIAl5ORFK1X1URBo4JYIk5OYY0XJPdxgiIklRIkhCk5yIHigTkQYv1EHnGruZX69n5tfr0x2GiEhSVCNIAXc1D4lIw6VEkISubZoCsEXDTIhIA6ZEkISrhvUB4Muv17Nuc1maoxERSYzuESShRX7s8p318IcAXHzoHtx48t5V4xCJiDQEqhEkoVm1D/wnP1zEvz5ZlqZoREQSo0SQhLxcXT4Rafj0SZaEmhKBRpwQkYZGiSAJxnc/9SPKBCLSwCgRJMH57vMDL32yjNUbS9IQjYhIYtRrKAk1PUf24fzVXPPsdHIjESbNK2bBb0+p/8BEROpANYIk7Oh54lUbSnl3bjHucO2z0/XksYhkNCWCJBxU0K7G7UXFG6uWX/70a6YvWVtPEYmI1J0SQRKa59XcslY5n3GlM//yAa989nV9hCQiUmdKBPVk6sI16Q5BRKRGSgT1JGLqVioimSnURGBmI8xsjpkVmdnoGvZfYWafm9mnZva+mQ0IM550euKDhRSMfo0R909KdygiItsJLRGYWQ7wIHASMAA4r4YP+qfdfT93HwT8HrgvrHgyxezlG9IdgojIdsKsERwMFLn7fHcvBZ4FRsUf4O7x03u1YMc9MjPW/j3a1Pk1H81fzUMTv6KiosH9uSLSCIX5QFl3YEnc+lJgaPWDzOwnwHVAHnBsiPGE4m+XHMSQ37xVp9ecO+YjAPbq0pL+XVvTqmkurZs2CSM8EZFdqlWNwMzya7MtEe7+oLv3AX4J3LKD819uZoVmVlhcXJyK06ZMy/zEc+kdr37J4fe8w8Db3+TtWSsAWLeljLvHzeKLZetSFaKIyE7V9lPsQ+CAWmyLtwzoGbfeI9i2I88CD9W0w93HAGMAhgwZklHtKU1yEm9dW7R6c9XyD54s3G7fmEnzufnkvfnRUb0TLl9EpDZ2+ilmZruZ2YFAMzMbbGYHBD/DgOa7KHsq0NfMeplZHnAuMLZa+X3jVk8B5tX1D0i3nBBHG71r3KzQyhYRqbSrGsGJwCXEvs3H9+hZD9y0sxe6e9TMrgbGAznAY+4+08zuAArdfSxwtZkNB8qAb4GLE/orGrE9bxpH0d0npzsMEWnEdpoI3P1J4Ekz+567v1jXwt19HDCu2rZb45avrWuZ2SaqnkUiErLaNnBPNrO/mdnrAGY2wMx+EGJcIiJST2qbCB4n1sTTLVifC/x3GAE1RE9cehCPXjQktPLfnLk8tLJFRGqbCDq6+3NABcTa/4Hy0KJqYIb160yP9s1CK//yv0/ji2XrWLu5lJJoOTO/Xsd9b84J7Xwikl1q2310k5l1IHjy18wOAdTRPU7XNuElAoBT//Q+AIfv2YGPF6yhrNy55ri+5CbRfVVEBGpfI7iOWNfPPmY2Gfg/4KehRdUAtWnWhIX3hD8t5eSi1ZSVx24glwczn01duIailRrDSEQSs8saQTB43NHBTz/AgDnuXhZybLILFRWx32c//CFAvSQiEWl8dlkjcPdy4Dx3j7r7THf/QkkgM/zhrbmURivSHYaINHB16T76ZzM7Mu7p4p0NL5G17j9nUL2da8yk+YyNmwLzly/MqLdzi0jjYe67fmDJzCbUsNndvd5HCx0yZIgXFhbu+sA0mrpwDSvWb+XUgd2Ys3wDt/z7c6Yu/LZezq3mIRGpiZlNc/ca+7nXqteQux+T2pAat4MK2lct99utFc9fcRgvTFvKL57/rN5ieGjiVxy+ZwcG9mhbb+cUkYapVonAzK6rYfM6YJq7f5rSiBqpsw7sUa+J4HdvzAZUQxCRXavtPYIhwBXEJpvpDvwYGAH81cxuCCk2SZBmPhORuqhtIugBHODuP3f3nwMHAp2Bo4iNTiq1cP85gxi+dxeuPmbP0M6xYNUmet80btcHiogEapsIOgMlcetlQBd331Jtu+zE6YO78+jFQzisT4fQzjH6xe17Dn2xbB23/PtzznroA9ZvVa9fEfmu2g4x8RQwxcxeDtZPA542sxbAl6FElgUO7d2BD+evrlrfp1trZn69PqkypyxYU7Vstm1oCoAJs1cyalD3pMoXkcantr2G7gyGoD482HSFu1f24bwglMgasV6dWgBw+uBuVYlg7m9OIidinP7gZDq0zGPinOTnZq7eM7h5XuLzK4tI41WXT4amwHp3f9zMOplZL3dfEFZgjVnXNs2Yf/fJRCLGL1/8HIC83Fgr3Ss/PQKATSVRTv3T+yxYtSll591SVk55hYc6vaaINDy17T56G7GeQ/2IzU3QBPgH22oIUkeR4MP4nZ8fzZay747o3SI/l04t81OaCK55ZjpL1mzmJyHerBaRhqe2N4vPAEYCmwDc/WugVVhBZZPenVqyT7c2Ne6rqMVT33X1cdw9BBERqH0iKPXYWBSV8xG0CC8kqdSj3Y7nOPjpsYl9q393bjGLVm/ig6JVXPi3KZTrmQORrFebYagNeNXMHgHamtmPgMuAv4YdXLa764z9KHd4JW5guUqjBnXjT+8UJVTu0fdOrFr+dnMpHVvms2j1Jrq0bkrTJjmJhisiDVRthqF24GzgBeBFYvcJbnX3P4UcW9ZrkZ/L6YO61bivR7vm9OrYgicuPSipc1Q2Px1970TO++tHSZUlIg1TbXsNfQKsdffrwwxGvuuYfp355Yj+tG6Wy9BesecO7v/PXJo2yWHCL4YlXf7HC9bwfOFSAKYvXstPnv6EB8/XCOMi2aS2w1DPBvYEFhHcMAZw94HhhVazhjAMdX0rGP1aSsu7+4z9KFy0hrvP2E9NRSKNRNLDUAMnpjAeSbHvHdCDFz9ZmrLybnop9mzD0Xt1qnoSednaLUyYvZLvH7JHys4jIpmhtk8WLwo7EEnc//7X/kQMnp+WumQAcO2zn7JXl1bs3bU1lz7+MXNXbOSkfXejQ8v8lJ5HRNKrtt1HJcPdefq+XHZ4r5SXe9ID7zFm0ld8uzk2YF1U3U1FGh0lgkaiaZMcbjq5P9ef2C/lZd89bja5wZPQSgQijY8SQSOSmxMJbfiIb9ZtBaBo5UYen7yAopUbKCuvCOVcIlK/atVrKOHCzUYADwA5wKPufk+1/dcBPwSiQDFw2a7uR6jXUO2kuidRjefo0JyJ12s6a5GGIBW9hhI5aQ7wIHA8sBSYamZj3T1+/oLpwBB332xmVwK/B84JK6ZsMubCA8nNMfrv1prD7nknlHMsXL0Zd2f91ihtmjUJ5RwiEr4wm4YOBorcfb67lwLPAqPiD3D3Ce6+OVj9iNiUmJICJ+yzG8f270K3ts0Yd82RoZ1nv9vfZP9fv8ni1ZurthWt3Mij780P7ZwiklphJoLuwJK49aXBth35AfB6TTvM7HIzKzSzwuLi5CdsyTZ7d23FbacNqFrv1qYpj15UYw2xzjaWRAE49n8nMm/FBgDO/MtkfvPaLKK6hyDSIGTElFVm9n1i8x0cXdN+dx8DjIHYPYJ6DK1RMDMuPbwXA7q2pmubZuzeoTkAVw7rw0MTv0rJOaIVzvF/mMRvz9yPDUFyKHfPjP9gIrJTYdYIlgE949Z7BNu2Y2bDgZuBke5eEmI8WW9o7w5VSQDg6hB6GN34r8+rpsjUENciDUOYiWAq0NfMeplZHnAuMDb+ADMbDDxCLAmsDDEWqUHYU1bqmQORhiG0mru7R83samA8se6jj7n7TDO7Ayh097HAvUBL4PnYtAcsdveRYcUk22vaJIdbTtmbw/p0ZO6KDSxYtQl3548JznNQ3cDb3+SVq49gvx41z8AmIpkh1CZcdx8HjKu27da45eFhnl927YdH9gZgQLfWVdtSlQgAPpq/WolAJMPpyWIJ3V63vM75mvRGJGMpEch3XHtc35SVtfTbzZRGK/jgq9UULlyTsnJFJHWUCOQ7fnb8XvRs3ywlZT354bYRQ856+MOUlCkiqaVEIDVaGww7HYYJs1cSLa+geEMJxRvUY1gk3ZQIpEZ3n7Ef/XdrxaMXDakagvrP5w9Outx35xZz6RNT+fOEIg666y0OuuutpMsUkeSEOvpoGDT6aHrd/NLn7NGhOecP3YN9bxufcDmH9enAB1+tBuCa4/py3fF7pSpEEanBzkYfVY1A6uSuM/bj8qP6kJ+b3H+dyiQA8Me35xH/heTixz5mwK1vJFW+iNSeEoEkJMdS+1Tyj/6vkE3BGEXvzi1mc2l5SssXkR3TmGCSkEhw3+BHR/bir+8tSLq8t2atZJ8kmppEJHFKBJKwhfecApCSRCAi6aOmIUnawntOYff2zXd9oIhkJCUCSYlJNxzDxzcdl9Iy7x0/m4bWq02kIVIikJTp3LopX919Mgt+ezJP/3Bo0uU9OOErlq3dkoLIRGRnlAgkpXIihplx2J4dUzKh/TMfL6Y0qikvRcKkRCChuezwXkmX8eCEr7j0iY9ZsX4rZZoDWSQUSgQSmmuH9+XxSw9KupzJRasZevfbDLt3YvJBich3KBFIqI7p15n5d5/M4Xt2SLqsZWu30O+W11MQlYjEUyKQ0EUixlM/PIQ3f3YUPz66d1JllUQrWLBqU4oiExFQIpB6tFeXVtx40t4ctVenpMo55n8m8utXZvL659+waLWSgkiy9GSx1LvKZwMiBn+54AC6tW3GyD9PrlMZj09eyOOTFwLbnnAWkcQoEUi9q3xG7IlLD66qHTRtEmFrmXoFiaSDmoak3lVU1Qi2jWBakUQOuPmlzylauTHZsESylhKB1LucYOTSSNxI1uXVhpLIidR+mOunpixm+H3vsrWsnPvenMNzU5cwZ/mGlMQqkg3UNCT17nffG8gj737Fwb3aV21r26wJqzeVVq1fc2xf/vDW3DqV2/9X209ms+C3J7OxJEqrpsk/4SzSmKlGIPWuW9tm/HrUvuTmbPvv98KVh5EXrI8+qT/XHLdn0ufpdeM49rv9Tb5Zp/GKRHZGiUAyQq+OLXjnF0czsEcbzj6wB2bGUz8cmpK5jJd9q0QgsjNKBJIxerRrztirj6BDy3wADt+zI9cc1zfpct+dW0zB6Ne46aXPAVi1sYRrnpleNTWmSLZTIpBG70/vFAHw9JTFrFi/lVF/nszYz75mn9vG8+bM5WmOTiT9Qk0EZjbCzOaYWZGZja5h/1Fm9omZRc3srDBjkYbr8UsP4rkfH5qSsobe/fZ2cxzc8/rslJQr0pCF1mvIzHKAB4HjgaXAVDMb6+5fxh22GLgE+EVYcUjDd0y/zgDs2bklZwzuzgVDd6dNsyZc8Y9pjJ+5IqmyN5eWs3j1ZjaXRem/W+tUhCvS4ITZffRgoMjd5wOY2bPAKKAqEbj7wmCfHimVXXrruqO3W3/kwiGURMvpd8sbO3jFri1fv5Wj7p0AwAejj+WI373D2KuPYN/ubZKKVaQhCbNpqDuwJG59abBNJGXyc3O4cliflJT19qwVVHhsVjSRbNIgbhab2eVmVmhmhcXFxekORzJMRYomuI9WxMqpy1PNIo1BmIlgGdAzbr1HsK3O3H2Muw9x9yGdOiU3hLE0PkcHA9f9+yeHJ1XOr1+JtVo+NWUxZz30QdJxiTQUYSaCqUBfM+tlZnnAucDYEM8nWeqwPh2Zf/fJDOrZloML2nPB0N0Z2CPxNv7yCqdw0bcUjH6N346bRf9fvU7xhpIURiySWUJLBO4eBa4GxgOzgOfcfaaZ3WFmIwHM7CAzWwqcDTxiZjPDikcat0jQnPPcFYdy1xn7cfvIfdi/Z1um3TI8qXIfmTSfrWUVfLxgTSrCFMlI5ilqX60vQ4YM8cLCwnSHIQ1IwejXki6jc6t8Vm4o4d6zBnL2kJ67foFIhjGzae4+pKZ9DeJmsUgynrj0IP56UY3//2ttZdA0dP0LM1IRkkhGUSKQRm9Yv84cP6ALC+85hU9+dXzV9htG9CORDkLPTV2y64NEGhA1DUnWmTB7Jeu3ljFqUHeKN5TwyLtf8ej7C+pcTtFdJ203lLZIJlPTkEicY/p3ZtSg2LONnVrlc8upA3g5ga6n475YTsHo13hp+lIqKhrWFyqReKoRiAQ2lkTZ97bxCb22X5dWjP/ZUSmOSCR1VCMQqYUWeTm0bppL2+Z1n9pyzooNPDihKISoRMKnOYtFAmbGjNtPBODlT5dx7bOf1un1946fw7otZUycs5K5KzZyzbF7ct0J/UKIVCS1VCMQqcGoQd05c3Ddx0gcM2k+c1dsBOCP76iGIA2DEoHIDtx62oCUTJUJsGZTKd+s28K/pyc03JZIqNQ0JLIDbZvncd3xezF8786M/PPkhMo46YH3uGpYH376zPSqbUN7t6drm2apClMkaaoRiOzCwB5tufP0fQG4clgfvvj1ibV+7axv1m+XBABO+9P7AETLK7jvzdh9BZF0UvdRkVpwd75Ytp794kY1HXbvBBau3pxQeWMuPJAb//U5qzeVAvDrkftw8WEFqQhVpEbqPiqSJDPbLgkAPPT9AznzgO6MvbruD6Nd/vdpVUkA4LaxM3c41PUXy9Zx3T8/ZWtZeZ3PI1IbqhGIpMjJD7zHl9+sT6qMQT3b0iTHuOqYPendsQXtWuRx0v3vsWztFgCevOzgqol4ROpiZzUC3SwWSZFx1x7J27NW8IMnY19UmuQYZeV1+6L16ZK1AFz6+NQa90+cs1KJQFJOTUMiKXTc3l0YuX83AH5/1kDaVXtKuWV+ct+9cmzbcKka30hSRTUCkRQ7f+jujP3saw7p3YFptxzP+JnLWbF+K5cc3otvN5Uy+M7/JFz2o+8voFenFtz80hcAfHzzcXRu1TRVoUuW0j0CkXpUEi2n3y1vpLTM4Xt34dGLh/DIu18xd8VGLjhkd9xhz84tufFfM3CPJacj+6pJKZvt7B6BEoFIPStcuIazHv6w3s973fF7cflRvWnaJKfezy3pp+6jIhlkSEF7psfNlFYp7JvA9/1nLv1/9QYvf7qMzaVRrnlmOn1vHkdD+zIoqadEIJIG7Vrk8eKVh263bfRJ/bdbv/uM/UI597XPfsqAW8cz9rOvKSt3xkyaz/TF3wLwp7fnUTD6NZasSexBOWmY1DQkkkbfrNvCI+/O55ZT9iY3J8L4mcv5Zu0WBnRrw8Aebej/qzc4qKAdUxd+W++xXXjIHtx5+r688cU3RMw4YZ/d6j0GSR3dIxBp4E74w7tVw1tfNawPf5n4Vb3H0KZZE342vC8XH1bAO7NX8pvXZrG5NMqUm4bXeyxSd0oEIg3cltJyNpdGKa9wOrduyl8mFtG7Y0v+9805zFu5Ma2x3Xn6vpw2sCttm+cx+sUZbCiJcv7Bu3PgHu249tnp5EYiRCLGoJ5t+cERvdIaazZTIhBppLaUlvP1ui28PWsFd4+bXeMxTZtE2FpWEXosh/Ruz0fz11StD9+7M2/NWrndMX07tyRixprNpdw5ah9G7Nv1O+UsXr0Zx9mjQ4vQY84mSgQiWWBjSRSIDW1RXuHMC5qSBvZow7otZZgZLfNz+fLr9Zz25/fTGWqVA/dox5kHdKd98zz+8NZcfnps36phuxfec0qao2tclAhEZDvPTV3CgG6tAXjg7Xl0b9uMJz5YCMTuBWTCHAl3jNqHT5es5faR+xAtdwoXrtEN6yQoEYhInbg7c1ds5PoXPmPpt1tYEzdkdjpN+MUwZixdy7XPflq17ci+HXlv3ipuGNGPY/p1Zt2WMgbv3pb8XD04F0+JQESSsm5zGRtKythYEuWDotXkRIzbxs5Md1i7NObCAyktr6Bw4bdcdOgeTJxTzB2vfgnAMz86hH67teKzJWuZsmANj01ewCMXHkjXNk3p2DKfDi3ysLhB/hq6tCUCMxsBPADkAI+6+z3V9ucD/wccCKwGznH3hTsrU4lAJDOc/uBkmuQYJdEKZixdB8BNJ/fnzAN6UBqtIFrudGqVz3//czqXH9WHmV+v49aXMz95VHftcX354KtVnHfw7uREjPfmreLeswYC8Mik+Zy2fze6t23GxpIod7wyk/67tebMA7rTtnkeAJOLVjG0V3tyc9L7/G5aEoGZ5QBzgeOBpcBU4Dx3/zLumKuAge5+hZmdC5zh7ufsrFwlApHMU/k5sqtv0PNWbOD5aUspjVaQlxvh7AN78I+PFvHM1CV874DuLFu7la1l5fRs15yOrfJYtGozH85fXXXPIi8nQml5rAdUXm6E0mj4vaF2pHvbZlUTBu1I+xZ52zWr9evSilZNc5m9fAMbS6IM7NGGAV1b07l1U/JzIxSt3MiqjSXkRGLXcd6KjSxbu4XdWjdlaO/2XHp4Lwb1bJtQvOlKBIcCt7v7icH6jQDu/tu4Y8YHx3xoZrnAcqCT7yQoJQKR7FMaraC0vIK8nAhNcoz1W6O0aRab6+HpKYuZsXQtW8vKGT9zBVvKyjlg97Z8snhteoMOwT1n7se5B++e0GvTNUNZd2BJ3PpSYOiOjnH3qJmtAzoAq+IPMrPLgcsBdt89sYsgIg1XXm6EvNxtTSuVSQBiQ2yfP3Tb50JlbSMa1BzMjGhFRdXN4+rfM80Md6/6DVASrcAdqldwyiuciBlmUBEcW+FgQH5uhAqPbS+vcFZvLKVTq3wiEXCP7d9cWs76rWUYRsSgRX4u5e60ys+ltLyChyZ+xVF7dWKvLq2ocGfNxlLuf2su3z9kD7q2bUbX1uHMPdEgJqZx9zHAGIjVCNIcjohksMqEEd8mnxPZ1oOopuarym2Vv1MxVHeLGmaja5GfW+N2gPzcHP57+F7bbWvdtAn3nzs46Vh2Jcy7F8uAnnHrPYJtNR4TNA21IXbTWERE6kmYiWAq0NfMeplZHnAuMLbaMWOBi4Pls4B3dnZ/QEREUi+0pqGgzf9qYDyx7qOPuftMM7sDKHT3scDfgL+bWRGwhliyEBGRehTqPQJ3HweMq7bt1rjlrcDZYcYgIiI7pxnKRESynBKBiEiWUyIQEclySgQiIlmuwY0+ambFwKIEX96Rak8tZyDFmLxMjw8yP8ZMjw8UY13t4e6datrR4BJBMsyscEdjbWQKxZi8TI8PMj/GTI8PFGMqqWlIRCTLKRGIiGS5bEsEY9IdQC0oxuRlenyQ+TFmenygGFMmq+4RiIjId2VbjUBERKpRIhARyXJZkwjMbISZzTGzIjMbnaYYeprZBDP70sxmmtm1wfb2ZvYfM5sX/G4XbDcz+2MQ8wwzO6AeY80xs+lm9mqw3svMpgSx/DMYWhwzyw/Wi4L9BfUQW1sze8HMZpvZLDM7NNOuoZn9LPg3/sLMnjGzpum+hmb2mJmtNLMv4rbV+bqZ2cXB8fPM7OKazpXC+O4N/p1nmNlLZtY2bt+NQXxzzOzEuO2hvddrijFu38/NzM2sY7Be79cwYe7e6H+IDYP9FdAbyAM+AwakIY6uwAHBcitgLjAA+D0wOtg+GvhdsHwy8DqxmfAOAabUY6zXAU8DrwbrzwHnBssPA1cGy1cBDwfL5wL/rIfYngR+GCznAW0z6RoSm4J1AdAs7tpdku5rCBwFHAB8EbetTtcNaA/MD363C5bbhRjfCUBusPy7uPgGBO/jfKBX8P7OCfu9XlOMwfaexIbcXwR0TNc1TPjvSufJ6+2PhEOB8XHrNwI3ZkBcLwPHA3OArsG2rsCcYPkR4Ly446uOCzmuHsDbwLHAq8F/5FVxb8iq6xn85z80WM4NjrMQY2sTfMhate0Zcw3ZNhd3++CavAqcmAnXECio9kFbp+sGnAc8Erd9u+NSHV+1fWcATwXL272HK69hfbzXa4oReAHYH1jItkSQlmuYyE+2NA1VvjErLQ22pU1Q/R8MTAG6uPs3wa7lQJdgOV1x3w/cAFQE6x2Ate4erSGOqhiD/euC48PSCygGHg+arh41sxZk0DV092XA/wCLgW+IXZNpZM41jFfX65bO99JlxL5hs5M46j0+MxsFLHP3z6rtypgYdyVbEkFGMbOWwIvAf7v7+vh9HvuKkLY+vWZ2KrDS3aelK4ZdyCVWNX/I3QcDm4g1aVTJgGvYDhhFLGl1A1oAI9IVT22l+7rtjJndDESBp9IdSzwzaw7cBNy6q2MzWbYkgmXE2vAq9Qi21Tsza0IsCTzl7v8KNq8ws67B/q7AymB7OuI+HBhpZguBZ4k1Dz0AtDWzyhnt4uOoijHY3wZYHWJ8S4Gl7j4lWH+BWGLIpGs4HFjg7sXuXgb8i9h1zZRrGK+u163er6eZXQKcClwQJKtMiq8PsYT/WfCe6QF8Yma7ZVCMu5QtiWAq0DfotZFH7Ibc2PoOwsyM2DzNs9z9vrhdY4HKngMXE7t3ULn9oqD3wSHAurhqfCjc/UZ37+HuBcSu0zvufgEwAThrBzFWxn5WcHxo3yrdfTmwxMz6BZuOA74kg64hsSahQ8ysefBvXhljRlzDaup63cYDJ5hZu6Dmc0KwLRRmNoJYM+VId99cLe5zgx5XvYC+wMfU83vd3T93987uXhC8Z5YS6xCynAy5hrWSzhsU9flD7A7+XGI9Cm5OUwxHEKt6zwA+DX5OJtYe/DYwD3gLaB8cb8CDQcyfA0PqOd5hbOs11JvYG60IeB7ID7Y3DdaLgv296yGuQUBhcB3/TaznRUZdQ+DXwGzgC+DvxHq3pPUaAs8Qu2dRRuwD6weJXDdibfVFwc+lIcdXRKw9vfL98nDc8TcH8c0BTorbHtp7vaYYq+1fyLabxfV+DRP90RATIiJZLluahkREZAeUCEREspwSgYhIllMiEBHJckoEIiJZTolAso6ZfRD8LjCz81Nc9k01nUskk6n7qGQtMxsG/MLdT63Da3J923hBNe3f6O4tUxCeSL1RjUCyjpltDBbvAY40s08tNn9ATjD+/dRg/PgfB8cPM7P3zGwssSeEMbN/m9k0i805cHmw7R6gWVDeU/HnCp4uvddi8xN8bmbnxJU90bbNr/BU8DQyZnaPxeaumGFm/1Of10iyS+6uDxFptEYTVyMIPtDXuftBZpYPTDazN4NjDwD2dfcFwfpl7r7GzJoBU83sRXcfbWZXu/ugGs51JrEnovcHOgavmRTsGwzsA3wNTAYON7NZxIZd7u/ubnETsoikmmoEItucQGxsmE+JDQ/egdgYNgAfxyUBgGvM7DPgI2IDiPVl544AnnH3cndfAbwLHBRX9lJ3ryA2jEIBsaGotwJ/M7Mzgc3fLVIkNZQIRLYx4KfuPij46eXulTWCTVUHxe4tDCc2mcz+wHRi4wUlqiRuuZzY5DVR4GBio6ueCryRRPkiO6VEINlsA7EpQyuNB64MhgrHzPYKJr2prg3wrbtvNrP+xKYhrFRW+fpq3gPOCe5DdCI25eHHOwrMYnNWtHH3ccDPiDUpiYRC9wgkm80AyoMmnieIzbtQQGw8eSM2E9rpNbzuDeCKoB1/DrHmoUpjgBlm9onHhu+u9BKxaRQ/IzYC7Q3uvjxIJDVpBbxsZk2J1VSuS+gvFKkFdR8VEclyahoSEclySgQiIllOiUBEJMspEYiIZDklAhGRLKdEICKS5ZQIRESy3P8DMHE9tzYydCoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(regret_lst)\n",
    "plt.ylabel('regret')\n",
    "plt.xlabel('iterations')\n",
    "plt.title(\"regret\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
